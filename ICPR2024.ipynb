{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510ac5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 11:57:31.457199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 11:57:31.570870: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-18 11:57:32.069863: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-18 11:57:32.069942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-18 11:57:32.069949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   #if like me you do not have a lot of memory in your GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" #then these two lines force keras to use your CPU\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import librosa\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from matplotlib import cm, colors, colorbar\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "rdg = RidgeClassifier(alpha=0.5)\n",
    "#mlp=MLPClassifier(random_state=1,max_iter=300,activation='relu',solver='sgd',learning_rate='constant',learning_rate_init=0.0001)\n",
    "mlp=MLPClassifier(random_state=1,max_iter=300,activation='relu')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr=LogisticRegression(random_state=1,max_iter=500)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(random_state=0,max_depth=10)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc= GradientBoostingClassifier(n_estimators=100, random_state=1)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#SGD=SGDClassifier(loss= 'log',random_state=1,max_iter=100,early_stopping=True,learning_rate='optimal',validation_fraction=0.2)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "mmscaler= MinMaxScaler()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "clf_svm=SVC(kernel='rbf')\n",
    "linear_svm=SVC(kernel='linear')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy.stats import skew\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "import random\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077863da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eeg(data):\n",
    "    with open(data) as f:\n",
    "        raw = []\n",
    "        for line in f:\n",
    "            line = line.split() # to deal with blank \n",
    "            if line:            # lines (ie skip them)\n",
    "                line = [int(float(i)) for i in line]\n",
    "                raw.append(line)\n",
    "    df = pd.DataFrame (raw,columns=['Channel_1','Channel_2','Channel_3','Channel_4','Channel_5','Channel_6','Channel_7','Channel_8',\n",
    "                            'Channel_9','Channel_10','Channel_11','Channel_12','Channel_13','Channel_14','Channel_15'\n",
    "                             ,'Channel_16','Channel_17','Channel_18','Channel_19'])\n",
    "    return df\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=6):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def noise_filter(df):\n",
    "    filt_data=pd.Series()\n",
    "    for i in range(0, df.shape[1]):\n",
    "        signal = df.iloc[:,i].values\n",
    "        data=butter_bandpass_filter(signal,0.1,45,128,3)\n",
    "        \n",
    "        filt_data=pd.concat([filt_data,pd.Series(data)],axis=1)\n",
    "    filt_data=filt_data.iloc[:,1:]\n",
    "    filt_data.columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
    "       '13', '14', '15', '16', '17', '18', '19']\n",
    "    return filt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e22f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tps=34944-5*128 # cut all EEG of all subjects into equal length = 34944-5*128=34304 time points\n",
    "number_of_epochs=26 # segment each subject's EEG into 26 epochs\n",
    "number_of_tps=1280 # each epoch include 1280 time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fccc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg_ob_0 (30, 34304, 19)\n",
      "eeg_l_0 (30, 34304, 19)\n",
      "X_0 (60, 34304, 19)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/jupy/Raw_data/Time_0/Obese\")\n",
    "ob_0=os.listdir(os.getcwd())\n",
    "ob_0.remove('.ipynb_checkpoints')\n",
    "#ob_0.remove('New')\n",
    "\n",
    "eeg_ob_0=np.zeros([len(ob_0),total_tps,19])\n",
    "for i in range(0,len(ob_0)):\n",
    "    eeg_ob_0[i,:,:]=(read_eeg(ob_0[i]).values)[0:total_tps,:]\n",
    "\n",
    "os.chdir(\"/home/jupy/Raw_data/Time_0/Lean\")\n",
    "l_0=os.listdir(os.getcwd())\n",
    "l_0.remove('.ipynb_checkpoints')\n",
    "#l_0.remove('New')\n",
    "\n",
    "eeg_l_0=np.zeros([len(l_0),total_tps,19])\n",
    "for i in range(0,len(l_0)):\n",
    "    eeg_l_0[i,:,:]=(read_eeg(l_0[i]).values)[0:total_tps,:]\n",
    "    \n",
    "X_0=np.vstack([eeg_ob_0,eeg_l_0])\n",
    "print ('eeg_ob_0',eeg_ob_0.shape)\n",
    "print ('eeg_l_0',eeg_l_0.shape)\n",
    "print('X_0',X_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4566824e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n",
      "/tmp/ipykernel_708216/877411679.py:27: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  filt_data=pd.Series()\n"
     ]
    }
   ],
   "source": [
    "y=[0]*30+[1]*30\n",
    "\n",
    "X_0_clean=np.zeros([60, 34304, 19])\n",
    "for i in range(0,X_0.shape[0]):\n",
    "    X_0_clean[i,:,:]=noise_filter(pd.DataFrame(X_0[i,:,:]))\n",
    "X_0_clean=X_0_clean[:,0:int(number_of_epochs*number_of_tps),:]\n",
    "\n",
    "X2_0=np.zeros([number_of_epochs*X_0_clean.shape[0],int(number_of_tps),19])\n",
    "for chan in range(0,19):\n",
    "    X1=np.zeros([number_of_epochs*X_0_clean.shape[0],int(number_of_tps)])\n",
    "    for subj in range(0,X_0_clean.shape[0]):\n",
    "        X1[subj*number_of_epochs:(subj+1)*number_of_epochs,:]=X_0_clean[subj,0:int(number_of_epochs*number_of_tps),chan].reshape(int(number_of_epochs),int(number_of_tps))\n",
    "    X2_0[:,:,chan]=X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60bc8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1560, 19, 1280)\n"
     ]
    }
   ],
   "source": [
    "XX_0=np.zeros([X2_0.shape[0],19,number_of_tps])\n",
    "for i in range(0,X2_0.shape[0]):\n",
    "    XX_0[i,:,:]=X2_0[i,:,:].transpose()\n",
    "print(XX_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792f30f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2=[]\n",
    "for n in y:\n",
    "    Y2=Y2+[n]*number_of_epochs\n",
    "len(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a2d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "del eeg_l_0\n",
    "del eeg_ob_0\n",
    "del X_0\n",
    "del X_0_clean\n",
    "del X2_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5771f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_short=[1,1,1,0,0,0]\n",
    "\n",
    "def subj_class_prob(result):\n",
    "    pred=[]\n",
    "    for i in range(0,6):\n",
    "        #print(np.where(np.array(result[i*number_of_epochs:(i+1)*number_of_epochs])==1)[0].shape[0]/number_of_epochs)\n",
    "        print(sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[0], sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[1])\n",
    "        if sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[0]>sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214afd1",
   "metadata": {},
   "source": [
    "# DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63853140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout,Add,LSTM,Reshape,Bidirectional\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D,AveragePooling1D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D,Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Dense, Lambda, Reshape, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D,Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Dense, Lambda, Reshape, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0965097",
   "metadata": {},
   "source": [
    "# VAE (build model + train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eba052e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "callbacks = EarlyStopping(monitor = 'loss',\n",
    "                          mode='min',\n",
    "                          patience =100,\n",
    "                          verbose = 0,\n",
    "                          restore_best_weights = True)\n",
    "encoded_weight={}\n",
    "for n in range(0,60):#2-48\n",
    "    latent_dim = 2\n",
    "    input_shape=(XX_0.shape[1:]) #(19,1280)\n",
    "    def sampling(args): \n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    ###################################  Encoder ################################## \n",
    "    inputs = Input(shape=(19,1280,1), name='encoder_input')\n",
    "    shape=(19,1280,1)\n",
    "    x = inputs\n",
    "    x = Conv2D(filters=8,kernel_size=(1, 64))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(filters=8,kernel_size=(19, 1),name='spatial_conv')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    initializer = tf.keras.initializers.TruncatedNormal(seed=0)\n",
    "    x = Dense(16, activation='relu',kernel_initializer=initializer)(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    z_log_var = z_log_var + 1e-8 \n",
    "\n",
    "    ###################################  Reparameterization ################################## \n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) \n",
    "\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    #encoder.summary()\n",
    "\n",
    "\n",
    "    ##################################  Decoder ################################## \n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(shape[0] * shape[1] * shape[2], activation='relu')(latent_inputs)\n",
    "    x = Reshape((shape[0], shape[1], shape[2]))(x)\n",
    "    x = Conv2DTranspose(filters=8,kernel_size=(19, 1),activation='relu',padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters=8,kernel_size=(1, 64),activation='relu',padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Conv2DTranspose(filters=1,kernel_size=5,padding='same',name='decoder_output')(x)\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    #decoder.summary()\n",
    "\n",
    "\n",
    "    ##################################  VAE model (merging encoder and decoder) ################################## \n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name='vae')    \n",
    "    callbacks = EarlyStopping(monitor = 'loss',\n",
    "                              mode='min',\n",
    "                              patience =20,\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights = True)\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)   \n",
    "\n",
    "    print(n)\n",
    "    xar_0=XX_0[n*26:(n+1)*26,:,:]\n",
    "    xar_norm_0=np.zeros([xar_0.shape[0],xar_0.shape[1],xar_0.shape[2]])\n",
    "    for i in range(0,xar_0.shape[0]):\n",
    "        for j in range(0,xar_0.shape[1]):\n",
    "            xar_norm_0[i,j,:]=minmax_scale(xar_0[i,j,:])\n",
    "    xar_0=xar_norm_0\n",
    "    xar_0=np.expand_dims(xar_0,axis=-1)\n",
    "    \n",
    "    # compiling vae\n",
    "    vae.compile(optimizer=Adam(learning_rate=0.0005), loss=None)\n",
    "    \n",
    "    vae.fit(xar_0,xar_0,\n",
    "            epochs=500 , # fit T0 and T15 in opposite order # Scalable - compile once\n",
    "            batch_size=10,\n",
    "            callbacks=callbacks)\n",
    "    \n",
    "    Encoder=Model(inputs, [z_mean, z_log_var, z], name='Encoder')\n",
    "    #encoded_weight[n]=Encoder.layers[4].get_weights()[0]\n",
    "    os.chdir('/home/jupy/ICPR2024/VAE')\n",
    "    tf.keras.models.save_model(Encoder,filepath=f'vae{n}',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10cab0e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "callbacks = EarlyStopping(monitor = 'loss',\n",
    "                          mode='min',\n",
    "                          patience =100,\n",
    "                          verbose = 0,\n",
    "                          restore_best_weights = True)\n",
    "encoded_weight={}\n",
    "for n in range(0,60):#2-48\n",
    "    print(n)\n",
    "    xar_0=XX_0[n*26:(n+1)*26,:,:]\n",
    "    xar_norm_0=np.zeros([xar_0.shape[0],xar_0.shape[1],xar_0.shape[2]])\n",
    "    for i in range(0,xar_0.shape[0]):\n",
    "        for j in range(0,xar_0.shape[1]):\n",
    "            xar_norm_0[i,j,:]=minmax_scale(xar_0[i,j,:])\n",
    "    xar_0=xar_norm_0\n",
    "    xar_0=np.expand_dims(xar_0,axis=-1)\n",
    "    \n",
    "    # compiling vae\n",
    "    vae.compile(optimizer=Adam(learning_rate=0.0005), loss=None)\n",
    "    \n",
    "    vae.fit(xar_0,xar_0,\n",
    "            epochs=500 , # fit T0 and T15 in opposite order # Scalable - compile once\n",
    "            batch_size=10,\n",
    "            callbacks=callbacks)\n",
    "    \n",
    "    Encoder=Model(inputs, [z_mean, z_log_var, z], name='Encoder')\n",
    "    #encoded_weight[n]=Encoder.layers[4].get_weights()[0]\n",
    "    os.chdir('/home/jupy/ICPR_vae')\n",
    "    tf.keras.models.save_model(Encoder,filepath=f'vae{n}',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b64c1",
   "metadata": {},
   "source": [
    "# --------------------Feature extract------------------------------------------------------ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164bac05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 11:58:19.758383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:19.766807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:19.767082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:19.768423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 11:58:19.768976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:19.769227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:19.769448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:20.174951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:20.175146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:20.175270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-18 11:58:20.175371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6830 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 11:58:21.155487: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 823ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 11:58:21.457942: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7eff18afa700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7eff0514b1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26, 1, 1217, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir('/home/jupy/ICPR_vae')\n",
    "os.chdir('/home/jupy/SavedModel/autoencoder/T0_to_T0')\n",
    "mm_feature_output_0_0={}\n",
    "for n in range(0,60):#2-48\n",
    "    xar_0=XX_0[n*26:(n+1)*26,:,:]\n",
    "    xar_norm_0=np.zeros([xar_0.shape[0],xar_0.shape[1],xar_0.shape[2]])\n",
    "    for i in range(0,xar_0.shape[0]):\n",
    "        for j in range(0,xar_0.shape[1]):\n",
    "            xar_norm_0[i,j,:]=minmax_scale(xar_0[i,j,:])\n",
    "    xar_0=xar_norm_0\n",
    "    xar_0=np.expand_dims(xar_0,axis=-1)\n",
    "    #mm=load_model(f'vae{n}')\n",
    "    mm=load_model(f'T0_to_T0_VAE{n}')\n",
    "    intermediate_layer_model = Model(inputs=mm.input,outputs=mm.get_layer('spatial_conv').output)\n",
    "    mm_feature_output_0_0[n]= intermediate_layer_model.predict(xar_0)\n",
    "mm_feature_output_0_0[n].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5973f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_auto_feat_0_0=np.zeros([26*60,1,1217,8])\n",
    "for n in range(0,60):\n",
    "    adapt_auto_feat_0_0[n*26:(n+1)*26,:,:,:]=mm_feature_output_0_0[n]\n",
    "Adapt_feat_0_0=np.zeros([26*60,1217*8])\n",
    "for i in range(0,adapt_auto_feat_0_0.shape[0]):\n",
    "    Adapt_feat_0_0[i,:]=adapt_auto_feat_0_0[i,:,:,:].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9c3cd",
   "metadata": {},
   "source": [
    "# TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a379f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xar_0=XX_0\n",
    "xar_norm_0=np.zeros([xar_0.shape[0],xar_0.shape[1],xar_0.shape[2]])\n",
    "for i in range(0,xar_0.shape[0]):\n",
    "        for j in range(0,xar_0.shape[1]):\n",
    "            xar_norm_0[i,j,:]=minmax_scale(xar_0[i,j,:])\n",
    "xar_0=xar_norm_0\n",
    "aaa=np.zeros([xar_0.shape[0],xar_0.shape[1]*xar_0.shape[2]])\n",
    "for i in range(0,xar_0.shape[0]):\n",
    "    aaa[i,:]=xar_0[i,:,:].flatten()\n",
    "xar_0=aaa\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne= TSNE(n_components=2, perplexity=15,random_state=0)\n",
    "tsne_raw=tsne.fit_transform(xar_0)\n",
    "tsne_vae=tsne.fit_transform(Adapt_feat_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b1427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### original VAE features - to delete latere ##############\n",
    "from sklearn.manifold import TSNE\n",
    "tsne= TSNE(n_components=2, perplexity=15)\n",
    "tsne_vae=tsne.fit_transform(Adapt_feat_0_0)\n",
    "\n",
    "##[0,10,22,29,55,40,50,59]\n",
    "\n",
    "for i,marker, c in zip ([0,5,10,50,55,59] , ['+','+','+','o','o','o'],['red','red','red','green','green','green'],\n",
    "                    ):\n",
    "    plt.scatter(tsne_vae[i*26:(i+1)*26,0],tsne_vae[i*26:(i+1)*26,1],alpha=0.3,marker=marker)#,color=c\n",
    "#plt.legend(loc=\"upper left\")\n",
    "####### original VAE features - to delete latere ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6634f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good subj [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "midhigh subj [18 19 20 21 22 23 24 25 26 34]\n",
      "midlow subj [27 28 30 47 53 54 56 57 58 59]\n",
      "bad subj [29 31 32 33 35 36 37 38 39 40 41 42 43 44 45 46 48 49 50 51 52 55]\n",
      "Accuracy: 0.8397435897435898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 5 # Number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(Adapt_feat_0_0)\n",
    "cluster_assignments = kmeans.labels_\n",
    "\n",
    "cluster_assignments.shape\n",
    "cluster_purity=[]\n",
    "for i in range(0,60):\n",
    "    cluster_count=(np.unique(cluster_assignments[i*26:(i+1)*26], return_counts=True)[0].shape[0])\n",
    "    cluster_purity.append(cluster_count)\n",
    "    \n",
    "counts, bins, _ = plt.hist(cluster_purity, bins=9)\n",
    "for count, x in zip(counts, bins):\n",
    "    if count > 0:  # Remove 0 count\n",
    "        plt.text(x + (bins[1] - bins[0]) / 2, count, str(int(count)), ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Number of Subjects')\n",
    "plt.xticks(range(1, 6))  # Set the x-axis to start from 1 to 9\n",
    "plt.yticks(range(0, 23, 5))  # Set the y-axis range from 0 to 22 with intervals of 5\n",
    "plt.show()\n",
    "good_q_id=np.where(np.array(cluster_purity)<=1.5)[0]\n",
    "midhigh_q_id = np.where((1.7 < np.array(cluster_purity)) & (np.array(cluster_purity) <=2.5))[0]\n",
    "midlow_q_id = np.where((2.5< np.array(cluster_purity)) & (np.array(cluster_purity) <= 3))[0]\n",
    "bad_q_id = np.where(np.array(cluster_purity)>3.5)[0]\n",
    "\n",
    "print('good subj',good_q_id)\n",
    "print('midhigh subj',midhigh_q_id)\n",
    "print('midlow subj',midlow_q_id)\n",
    "print('bad subj',bad_q_id)\n",
    "\n",
    "quality_y=np.empty([1560,])\n",
    "#quality_ar=np.zeros[1560,9736]\n",
    "for i in good_q_id:\n",
    "    #print(np.arange(i*26,(i+1)*26))\n",
    "    quality_y[np.arange(i*26,(i+1)*26)]=0\n",
    "    \n",
    "for j in midhigh_q_id:\n",
    "    quality_y[np.arange(j*26,(j+1)*26)]=1\n",
    "    \n",
    "for r in midlow_q_id:\n",
    "    quality_y[np.arange(r*26,(r+1)*26)]=1\n",
    "    \n",
    "for z in bad_q_id:\n",
    "    quality_y[np.arange(z*26,(z+1)*26)]=2\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Number of trees in the Random Forest\n",
    "n_estimators = 200\n",
    "\n",
    "# Create a Random Forest classifier with bootstrapping\n",
    "rf = RandomForestClassifier(n_estimators=n_estimators, bootstrap=True)\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(Adapt_feat_0_0, quality_y, test_size=0.2, random_state=42,stratify=quality_y\n",
    ")#, \n",
    "# Train the Random Forest model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "rf.feature_importances_\n",
    "feat_id=rf.feature_importances_.argsort()[-500:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5233cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/jupy/.conda/envs/venv_py39/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne= TSNE(n_components=2, perplexity=15)\n",
    "tsne_vae=tsne.fit_transform(Adapt_feat_0_0[:,feat_id])\n",
    "#tsne_vae=tsne.fit_transform(xar_0[:,feat_id])\n",
    "\n",
    "for i,marker, c in zip ([0,15,21,27,37,43,52,59] , ['+','+','+','+','o','o','o','o'],['red','red','red','red','green','green','green','green'],                  ):\n",
    "    plt.scatter(tsne_vae[i*26:(i+1)*26,0],tsne_vae[i*26:(i+1)*26,1],alpha=0.7,marker=marker)#,color=c\n",
    "#plt.legend(labels=['Lean', 'Obese'], loc='upper right')\n",
    "os.chdir('/home/jupy/ICPR_plots')\n",
    "plt.savefig(r'tsne_samp_vae.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea36bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d748dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd318d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_feat=tsne_raw\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==0)[0],0],tsne_feat[np.where(np.array(Y2)==0)[0],1],alpha=0.3,label=\"Obese\")\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==1)[0],0],tsne_feat[np.where(np.array(Y2)==1)[0],1],alpha=0.3,label=\"Lean\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.gcf()\n",
    "os.chdir('/home/jupy/ICPR2024/plots')\n",
    "#plt.savefig(r'tsne_raw.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046dc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_feat=tsne_raw\n",
    "for subj,marker in zip( [7,10,2,16,25,35,39,43,56,58],['+','+','+','+','+','o','o','o','o','o']):\n",
    "    plt.scatter(tsne_feat[subj*number_of_epochs:(subj+1)*number_of_epochs,0],\n",
    "                tsne_feat[subj*number_of_epochs:(subj+1)*number_of_epochs,1],alpha=0.8, marker=marker)\n",
    "#plt.savefig(r'tsne_raw_10subj.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb42316",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_feat=tsne_vae\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==0)[0],0],tsne_feat[np.where(np.array(Y2)==0)[0],1],alpha=0.3,label=\"Obese\")\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==1)[0],0],tsne_feat[np.where(np.array(Y2)==1)[0],1],alpha=0.3,label=\"Lean\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.gcf()\n",
    "os.chdir('/home/jupy/ICPR2024/plots')\n",
    "#plt.savefig(r'tsne_vae.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_feat=tsne_vae\n",
    "for subj,marker in zip( [0,5,11,19,16,26,34,37,48,56,59],['+','+','+','+','o','o','o','o']):\n",
    "    plt.scatter(tsne_feat[subj*number_of_epochs:(subj+1)*number_of_epochs,0],\n",
    "                tsne_feat[subj*number_of_epochs:(subj+1)*number_of_epochs,1],alpha=0.8, marker=marker)\n",
    "#plt.savefig(r'tsne_vae_10subj.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c,lab in zip ( [2,5,18,21,24,31,40,56], ['red','red','red','red','red','green','green','green'],\n",
    "                    [\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"lean\",\"lean\",\"lean\"]):\n",
    "    plt.scatter(tsne_feat_dann[i*26:(i+1)*26,0],tsne_feat_dann[i*26:(i+1)*26,1],alpha=0.3,c=c,label=lab)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e9d36",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd25a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1560, 19, 1280)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7baf64c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set 0 [30 31 32  0  1  2]\n",
      "set 0 [33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 1 [33 34 35  3  4  5]\n",
      "set 1 [30, 31, 32, 0, 1, 2, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 2 [36 37 38  6  7  8]\n",
      "set 2 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 3 [39 40 41  9 10 11]\n",
      "set 3 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 4 [42 43 44 12 13 14]\n",
      "set 4 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 5 [45 46 47 15 16 17]\n",
      "set 5 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 6 [48 49 50 18 19 20]\n",
      "set 6 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 7 [51 52 53 21 22 23]\n",
      "set 7 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 8 [54 55 56 24 25 26]\n",
      "set 8 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 57, 58, 59, 27, 28, 29]\n",
      "----------------------------\n",
      "set 9 [57 58 59 27 28 29]\n",
      "set 9 [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26]\n",
      "----------------------------\n",
      "set 0\n",
      "test_id [30 31 32  0  1  2]\n",
      "tv_id [33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 1\n",
      "test_id [33 34 35  3  4  5]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 2\n",
      "test_id [36 37 38  6  7  8]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 3\n",
      "test_id [39 40 41  9 10 11]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 4\n",
      "test_id [42 43 44 12 13 14]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 5\n",
      "test_id [45 46 47 15 16 17]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 6\n",
      "test_id [48 49 50 18 19 20]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 7\n",
      "test_id [51 52 53 21 22 23]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 54, 55, 56, 24, 25, 26, 57, 58, 59, 27, 28, 29]\n",
      "set 8\n",
      "test_id [54 55 56 24 25 26]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 57, 58, 59, 27, 28, 29]\n",
      "set 9\n",
      "test_id [57 58 59 27 28 29]\n",
      "tv_id [30, 31, 32, 0, 1, 2, 33, 34, 35, 3, 4, 5, 36, 37, 38, 6, 7, 8, 39, 40, 41, 9, 10, 11, 42, 43, 44, 12, 13, 14, 45, 46, 47, 15, 16, 17, 48, 49, 50, 18, 19, 20, 51, 52, 53, 21, 22, 23, 54, 55, 56, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "data_ind=[]\n",
    "for n in range(0,10):\n",
    "    data_ind.append(np.array([30,31,32,0,1,2])+n*3)\n",
    "\n",
    "Xtest_id={}\n",
    "Xtv_id={}\n",
    "\n",
    "for s in range(0,10):    \n",
    "    Xtest_id[s]=data_ind[s]\n",
    "    #Xtv_id[s]=flatten(data_ind[0:s]+data_ind[(s+1):])\n",
    "    Xtv_id[s]=(np.array(data_ind[0:s]).flatten().tolist())+(np.array(data_ind[(s+1):]).flatten().tolist())\n",
    "    print('set',s,Xtest_id[s])\n",
    "    print('set',s,Xtv_id[s])\n",
    "    print('----------------------------')\n",
    "    \n",
    "Xdata= XX_0#Adapt_feat_0_0\n",
    "Ydata=np.array(Y2)\n",
    "Xtv={}\n",
    "Ytv={}\n",
    "Xtest={}\n",
    "Ytest={}\n",
    "for s in range(0,10):\n",
    "    print('set',s)\n",
    "    Xtest[s]=np.zeros([Xdata.shape[0],Xdata.shape[1],Xdata.shape[2] ])#\n",
    "    Ytest[s]=np.zeros([Xdata.shape[0],])\n",
    "    print('test_id',Xtest_id[s])\n",
    "    for test_id in Xtest_id[s]:\n",
    "            Xtest[s]=np.vstack([Xtest[s],Xdata[test_id*number_of_epochs:(test_id+1)*number_of_epochs,:,:]])\n",
    "            Ytest[s]=np.hstack([Ytest[s],Ydata[test_id*number_of_epochs:(test_id+1)*number_of_epochs,]])\n",
    "          \n",
    "    Xtest[s]=Xtest[s][Xdata.shape[0]:,:,:] \n",
    "    Ytest[s]=Ytest[s][Xdata.shape[0]:,] \n",
    "    \n",
    "    Xtv[s]=np.zeros([Xdata.shape[0],Xdata.shape[1],Xdata.shape[2]])#\n",
    "    Ytv[s]=np.zeros([Xdata.shape[0],])\n",
    "    print('tv_id',Xtv_id[s])\n",
    "    for tv_id in Xtv_id[s]:\n",
    "            Xtv[s]=np.vstack([Xtv[s],Xdata[tv_id*number_of_epochs:(tv_id+1)*number_of_epochs,:,:]])\n",
    "            Ytv[s]=np.hstack([Ytv[s],Ydata[tv_id*number_of_epochs:(tv_id+1)*number_of_epochs,]])\n",
    "            \n",
    "    Xtv[s]=Xtv[s][Xdata.shape[0]:,:,:] \n",
    "    Ytv[s]=Ytv[s][Xdata.shape[0]:,] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1c2875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n",
      "(1404, 19, 1280)\n",
      "(1404,)\n"
     ]
    }
   ],
   "source": [
    "all_id_long={}\n",
    "X_all_shuffled={}\n",
    "Y_all_shuffled={}\n",
    "Y_all_shuffled_cat={}\n",
    "for s in range(0,10):\n",
    "    np.random.seed(2)\n",
    "    all_id_long[s]=np.arange(0,Xtv[s].shape[0])\n",
    "    np.random.shuffle(all_id_long[s])\n",
    "    \n",
    "    X_all_shuffled[s]=np.zeros([ Xtv[s].shape[0], Xtv[s].shape[1], Xtv[s].shape[2]])\n",
    "    Y_all_shuffled[s]=np.zeros([ Xtv[s].shape[0]])\n",
    "    for n in range(0,all_id_long[s].shape[0]):\n",
    "        X_all_shuffled[s][n,:]=Xtv[s][all_id_long[s][n],:,:]\n",
    "        #print(n,all_id_long[s][n])\n",
    "        Y_all_shuffled[s][n]=Ytv[s][all_id_long[s][n]]\n",
    "    print(X_all_shuffled[s].shape)\n",
    "    print(Y_all_shuffled[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82cb33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(0,10):\n",
    "    del  Xtv[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39b92b",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ffc1fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1280)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75342397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 19, 1280)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d_195 (Conv1D)            (None, 19, 64)       81984       ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_196 (Conv1D)            (None, 19, 64)       245824      ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_197 (Conv1D)            (None, 19, 64)       409664      ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 19, 192)      0           ['conv1d_195[0][0]',             \n",
      "                                                                  'conv1d_196[0][0]',             \n",
      "                                                                  'conv1d_197[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_198 (Conv1D)            (None, 19, 1)        193         ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " multiply_17 (Multiply)         (None, 19, 192)      0           ['concatenate_17[0][0]',         \n",
      "                                                                  'conv1d_198[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_16 (G  (None, 192)         0           ['multiply_17[0][0]']            \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 64)           12352       ['global_average_pooling1d_16[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 2)            130         ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 750,147\n",
      "Trainable params: 750,147\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, concatenate, Add, GlobalAveragePooling1D, Dense, Multiply, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def attention_vgg_residual_inception(input_shape, filters, num_classes):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # VGG Block\n",
    "    vgg_conv1 = Conv1D(filters[0], 3, activation='relu', padding='same')(input_layer)\n",
    "    vgg_conv2 = Conv1D(filters[1], 3, activation='relu', padding='same')(vgg_conv1)\n",
    "    \n",
    "    # Residual Block\n",
    "    residual_conv1 = Conv1D(filters[2], 1, activation='relu', padding='same')(input_layer)\n",
    "    residual_conv2 = Conv1D(filters[3], 3, activation='relu', padding='same')(residual_conv1)\n",
    "    residual_conv3 = Conv1D(filters[4], 1, activation='relu', padding='same')(residual_conv2)\n",
    "    \n",
    "    # Inception Block\n",
    "    inception_conv1 = Conv1D(filters[5], 1, activation='relu', padding='same')(input_layer)\n",
    "    inception_conv3 = Conv1D(filters[6], 3, activation='relu', padding='same')(input_layer)\n",
    "    inception_conv5 = Conv1D(filters[7], 5, activation='relu', padding='same')(input_layer)\n",
    "    inception_output = concatenate([inception_conv1, inception_conv3, inception_conv5], axis=-1)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention_weights = Conv1D(1, 1, activation='softmax', padding='same')(inception_output)\n",
    "    attention_output = Multiply()([inception_output, attention_weights])\n",
    "    \n",
    "    # Reshape to match the number of channels\n",
    "    residual_conv1 = Conv1D(filters[4], 1, activation='relu', padding='same')(residual_conv1)\n",
    "    \n",
    "    # Element-wise addition\n",
    "    residual_output = Add()([residual_conv3, residual_conv1])\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    gap = GlobalAveragePooling1D()(attention_output)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fc1 = Dense(64, activation='relu')(gap)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='sigmoid')(fc1)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (19, 1280)\n",
    "filters = [64, 128, 64, 128, 64, 64, 64, 64]  # Example filter sizes, adjust as needed\n",
    "num_classes = 2  # Binary classification\n",
    "vgg_mod= attention_vgg_residual_inception(input_shape, filters, num_classes)\n",
    "vgg_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ebaa2cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 5.3822 - accuracy: 0.5102 - val_loss: 4.0602 - val_accuracy: 0.5125\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.2513 - accuracy: 0.5708 - val_loss: 3.4717 - val_accuracy: 0.5089\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 2.2213 - accuracy: 0.6180 - val_loss: 3.0965 - val_accuracy: 0.5267\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 4s 799ms/step - loss: 1.5017 - accuracy: 0.6812 - val_loss: 3.0318 - val_accuracy: 0.5160\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0649 - accuracy: 0.7418 - val_loss: 2.7678 - val_accuracy: 0.5445\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.7282 - accuracy: 0.7890 - val_loss: 2.7263 - val_accuracy: 0.5302\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 0.4816 - accuracy: 0.8513 - val_loss: 2.8100 - val_accuracy: 0.5231\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 2s 306ms/step - loss: 0.3351 - accuracy: 0.8869 - val_loss: 2.7341 - val_accuracy: 0.5374\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 2s 474ms/step - loss: 0.2397 - accuracy: 0.9172 - val_loss: 2.6834 - val_accuracy: 0.5338\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1598 - accuracy: 0.9528 - val_loss: 2.7304 - val_accuracy: 0.5267\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1086 - accuracy: 0.9724 - val_loss: 2.7094 - val_accuracy: 0.5196\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 0.0790 - accuracy: 0.9831 - val_loss: 2.7251 - val_accuracy: 0.5196\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0580 - accuracy: 0.9893 - val_loss: 2.7443 - val_accuracy: 0.5302\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0466 - accuracy: 0.9938 - val_loss: 2.6892 - val_accuracy: 0.5338\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 0.0347 - accuracy: 0.9973 - val_loss: 2.6890 - val_accuracy: 0.5267\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0292 - accuracy: 0.9991 - val_loss: 2.7134 - val_accuracy: 0.5302\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 0.0249 - accuracy: 0.9991 - val_loss: 2.7081 - val_accuracy: 0.5302\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 3s 625ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 2.7129 - val_accuracy: 0.5338\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 2.7153 - val_accuracy: 0.5302\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 2.7226 - val_accuracy: 0.5267\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 2.7209 - val_accuracy: 0.5267\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.7216 - val_accuracy: 0.5338\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.7248 - val_accuracy: 0.5302\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 2.7293 - val_accuracy: 0.5302\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 2.7287 - val_accuracy: 0.5267\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 2.7339 - val_accuracy: 0.5302\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.7369 - val_accuracy: 0.5267\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 2.7410 - val_accuracy: 0.5302\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.7442 - val_accuracy: 0.5267\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.7459 - val_accuracy: 0.5302\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.7522 - val_accuracy: 0.5338\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.7582 - val_accuracy: 0.5267\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.7586 - val_accuracy: 0.5267\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.7559 - val_accuracy: 0.5302\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.7551 - val_accuracy: 0.5338\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.7586 - val_accuracy: 0.5374\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.7660 - val_accuracy: 0.5302\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.7648 - val_accuracy: 0.5338\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.7652 - val_accuracy: 0.5374\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.7711 - val_accuracy: 0.5338\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 2s 430ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.7759 - val_accuracy: 0.5267\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.7780 - val_accuracy: 0.5267\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.7820 - val_accuracy: 0.5267\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.7819 - val_accuracy: 0.5445\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.7866 - val_accuracy: 0.5302\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.7865 - val_accuracy: 0.5338\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.7893 - val_accuracy: 0.5302\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 199ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.7890 - val_accuracy: 0.5338\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.7930 - val_accuracy: 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 5.9018 - accuracy: 0.5138 - val_loss: 4.5992 - val_accuracy: 0.5338\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.5535 - accuracy: 0.6020 - val_loss: 4.4812 - val_accuracy: 0.5231\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.1360 - accuracy: 0.6527 - val_loss: 4.5857 - val_accuracy: 0.5302\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.6393 - accuracy: 0.6990 - val_loss: 4.2581 - val_accuracy: 0.5445\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0963 - accuracy: 0.7489 - val_loss: 4.0241 - val_accuracy: 0.5018\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6831 - accuracy: 0.8112 - val_loss: 3.9084 - val_accuracy: 0.4982\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4520 - accuracy: 0.8727 - val_loss: 3.8221 - val_accuracy: 0.4875\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2955 - accuracy: 0.9012 - val_loss: 3.7626 - val_accuracy: 0.4911\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1830 - accuracy: 0.9377 - val_loss: 3.7580 - val_accuracy: 0.5053\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1162 - accuracy: 0.9662 - val_loss: 3.7532 - val_accuracy: 0.5018\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0789 - accuracy: 0.9840 - val_loss: 3.7693 - val_accuracy: 0.5018\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0515 - accuracy: 0.9938 - val_loss: 3.6888 - val_accuracy: 0.5053\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0384 - accuracy: 0.9947 - val_loss: 3.6642 - val_accuracy: 0.4911\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0311 - accuracy: 0.9973 - val_loss: 3.6855 - val_accuracy: 0.5018\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0256 - accuracy: 0.9982 - val_loss: 3.6440 - val_accuracy: 0.5018\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0187 - accuracy: 0.9991 - val_loss: 3.6442 - val_accuracy: 0.5089\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 3.6022 - val_accuracy: 0.5053\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 3.5980 - val_accuracy: 0.5089\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.6246 - val_accuracy: 0.5125\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.6108 - val_accuracy: 0.5089\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.5846 - val_accuracy: 0.5125\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.5880 - val_accuracy: 0.5089\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.5897 - val_accuracy: 0.5125\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.5890 - val_accuracy: 0.5089\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.5818 - val_accuracy: 0.5125\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.5871 - val_accuracy: 0.5125\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.5923 - val_accuracy: 0.5125\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.5922 - val_accuracy: 0.5160\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.5970 - val_accuracy: 0.5160\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.5940 - val_accuracy: 0.5125\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.5907 - val_accuracy: 0.5053\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5970 - val_accuracy: 0.5089\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.5925 - val_accuracy: 0.5125\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.5931 - val_accuracy: 0.5125\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5962 - val_accuracy: 0.5125\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.5944 - val_accuracy: 0.5125\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5950 - val_accuracy: 0.5089\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.5962 - val_accuracy: 0.5125\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5919 - val_accuracy: 0.5089\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.5932 - val_accuracy: 0.5089\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.5961 - val_accuracy: 0.5089\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.5944 - val_accuracy: 0.5089\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.5919 - val_accuracy: 0.5089\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5939 - val_accuracy: 0.5089\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.5957 - val_accuracy: 0.5089\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.5966 - val_accuracy: 0.5089\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.5918 - val_accuracy: 0.5089\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.5915 - val_accuracy: 0.5089\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.5959 - val_accuracy: 0.5089\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.5935 - val_accuracy: 0.5089\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.5926 - val_accuracy: 0.5089\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.5929 - val_accuracy: 0.5125\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.5974 - val_accuracy: 0.5125\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.5959 - val_accuracy: 0.5125\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.5924 - val_accuracy: 0.5125\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.5937 - val_accuracy: 0.5125\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5970 - val_accuracy: 0.5125\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5981 - val_accuracy: 0.5125\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5994 - val_accuracy: 0.5125\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.6035 - val_accuracy: 0.5125\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.6006 - val_accuracy: 0.5125\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.5982 - val_accuracy: 0.5089\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.6036 - val_accuracy: 0.5089\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.6017 - val_accuracy: 0.5089\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.6008 - val_accuracy: 0.5089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 7.8039 - accuracy: 0.5298 - val_loss: 7.5263 - val_accuracy: 0.4875\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.8474 - accuracy: 0.5476 - val_loss: 4.9915 - val_accuracy: 0.5018\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.3070 - accuracy: 0.5922 - val_loss: 4.8582 - val_accuracy: 0.5089\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.3464 - accuracy: 0.6465 - val_loss: 4.8643 - val_accuracy: 0.5196\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.7530 - accuracy: 0.6839 - val_loss: 4.1325 - val_accuracy: 0.5302\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 276ms/step - loss: 1.2575 - accuracy: 0.7257 - val_loss: 3.9598 - val_accuracy: 0.5302\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.8886 - accuracy: 0.7809 - val_loss: 4.0107 - val_accuracy: 0.4947\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.6368 - accuracy: 0.8183 - val_loss: 3.7469 - val_accuracy: 0.5196\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4276 - accuracy: 0.8673 - val_loss: 3.6503 - val_accuracy: 0.5267\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2951 - accuracy: 0.9029 - val_loss: 3.6905 - val_accuracy: 0.4911\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2054 - accuracy: 0.9350 - val_loss: 3.5858 - val_accuracy: 0.4982\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 2s 306ms/step - loss: 0.1371 - accuracy: 0.9564 - val_loss: 3.5311 - val_accuracy: 0.5196\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1029 - accuracy: 0.9733 - val_loss: 3.5367 - val_accuracy: 0.5196\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.0720 - accuracy: 0.9902 - val_loss: 3.5542 - val_accuracy: 0.5053\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0545 - accuracy: 0.9929 - val_loss: 3.5099 - val_accuracy: 0.5231\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0424 - accuracy: 0.9973 - val_loss: 3.4758 - val_accuracy: 0.5231\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0355 - accuracy: 0.9982 - val_loss: 3.4696 - val_accuracy: 0.5160\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0296 - accuracy: 0.9982 - val_loss: 3.4873 - val_accuracy: 0.5302\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0248 - accuracy: 0.9991 - val_loss: 3.4963 - val_accuracy: 0.5302\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0214 - accuracy: 0.9991 - val_loss: 3.4871 - val_accuracy: 0.5302\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 3.4754 - val_accuracy: 0.5196\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 3.4731 - val_accuracy: 0.5196\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.4769 - val_accuracy: 0.5231\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.4876 - val_accuracy: 0.5267\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 3.4929 - val_accuracy: 0.5231\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 3.4956 - val_accuracy: 0.5196\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 3.4963 - val_accuracy: 0.5160\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 3.5007 - val_accuracy: 0.5160\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.5032 - val_accuracy: 0.5160\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.5068 - val_accuracy: 0.5160\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 3.5078 - val_accuracy: 0.5160\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.5102 - val_accuracy: 0.5196\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.5127 - val_accuracy: 0.5160\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.5143 - val_accuracy: 0.5196\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 1s 244ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.5160 - val_accuracy: 0.5196\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.5193 - val_accuracy: 0.5196\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.5221 - val_accuracy: 0.5196\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.5219 - val_accuracy: 0.5160\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.5220 - val_accuracy: 0.5196\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.5230 - val_accuracy: 0.5196\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.5254 - val_accuracy: 0.5196\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5289 - val_accuracy: 0.5231\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.5324 - val_accuracy: 0.5231\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.5316 - val_accuracy: 0.5231\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.5318 - val_accuracy: 0.5231\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5333 - val_accuracy: 0.5231\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.5351 - val_accuracy: 0.5231\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.5367 - val_accuracy: 0.5196\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.5383 - val_accuracy: 0.5231\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.5402 - val_accuracy: 0.5231\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5410 - val_accuracy: 0.5196\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.5412 - val_accuracy: 0.5231\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5424 - val_accuracy: 0.5196\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.5451 - val_accuracy: 0.5196\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.5480 - val_accuracy: 0.5196\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5485 - val_accuracy: 0.5196\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.5492 - val_accuracy: 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 6.5032 - accuracy: 0.5004 - val_loss: 3.6268 - val_accuracy: 0.5196\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.8299 - accuracy: 0.5539 - val_loss: 3.2957 - val_accuracy: 0.5623\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.5963 - accuracy: 0.6109 - val_loss: 3.9449 - val_accuracy: 0.5374\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.2001 - accuracy: 0.6492 - val_loss: 3.7675 - val_accuracy: 0.5302\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.5879 - accuracy: 0.7186 - val_loss: 3.9950 - val_accuracy: 0.5552\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.1685 - accuracy: 0.7489 - val_loss: 3.7891 - val_accuracy: 0.5409\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.7526 - accuracy: 0.8139 - val_loss: 3.5256 - val_accuracy: 0.5053\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4915 - accuracy: 0.8504 - val_loss: 3.6513 - val_accuracy: 0.5231\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3181 - accuracy: 0.8994 - val_loss: 3.6053 - val_accuracy: 0.5267\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2131 - accuracy: 0.9350 - val_loss: 3.5881 - val_accuracy: 0.5231\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1365 - accuracy: 0.9564 - val_loss: 3.7081 - val_accuracy: 0.5302\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0870 - accuracy: 0.9831 - val_loss: 3.6503 - val_accuracy: 0.5302\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0598 - accuracy: 0.9902 - val_loss: 3.6421 - val_accuracy: 0.5374\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0420 - accuracy: 0.9938 - val_loss: 3.5743 - val_accuracy: 0.5338\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0494 - accuracy: 0.9929 - val_loss: 3.3290 - val_accuracy: 0.5374\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0447 - accuracy: 0.9947 - val_loss: 3.2554 - val_accuracy: 0.5409\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0414 - accuracy: 0.9964 - val_loss: 3.5096 - val_accuracy: 0.5231\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0350 - accuracy: 0.9964 - val_loss: 3.4305 - val_accuracy: 0.5374\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0260 - accuracy: 0.9982 - val_loss: 3.3932 - val_accuracy: 0.5231\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0225 - accuracy: 0.9982 - val_loss: 3.4632 - val_accuracy: 0.5338\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 3.5186 - val_accuracy: 0.5374\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.5213 - val_accuracy: 0.5302\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 3.4873 - val_accuracy: 0.5338\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 3.4652 - val_accuracy: 0.5445\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 3.4821 - val_accuracy: 0.5409\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.4991 - val_accuracy: 0.5409\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.5053 - val_accuracy: 0.5409\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.5065 - val_accuracy: 0.5480\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.5063 - val_accuracy: 0.5480\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.5067 - val_accuracy: 0.5480\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.5069 - val_accuracy: 0.5480\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.5055 - val_accuracy: 0.5480\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.5053 - val_accuracy: 0.5480\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5068 - val_accuracy: 0.5480\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.5117 - val_accuracy: 0.5445\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.5119 - val_accuracy: 0.5480\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.5095 - val_accuracy: 0.5480\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.5110 - val_accuracy: 0.5480\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.5149 - val_accuracy: 0.5480\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.5185 - val_accuracy: 0.5480\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.5232 - val_accuracy: 0.5480\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.5224 - val_accuracy: 0.5480\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5185 - val_accuracy: 0.5480\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.5219 - val_accuracy: 0.5480\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.5246 - val_accuracy: 0.5516\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.5255 - val_accuracy: 0.5516\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.5300 - val_accuracy: 0.5516\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.5288 - val_accuracy: 0.5516\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.5275 - val_accuracy: 0.5480\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.5288 - val_accuracy: 0.5480\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.5318 - val_accuracy: 0.5516\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5330 - val_accuracy: 0.5445\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.5342 - val_accuracy: 0.5480\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.5355 - val_accuracy: 0.5480\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.5354 - val_accuracy: 0.5445\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.5378 - val_accuracy: 0.5480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 6.0156 - accuracy: 0.4915 - val_loss: 5.3632 - val_accuracy: 0.5196\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.9069 - accuracy: 0.5352 - val_loss: 4.2266 - val_accuracy: 0.4875\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.9361 - accuracy: 0.5726 - val_loss: 3.9893 - val_accuracy: 0.4804\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.0487 - accuracy: 0.6447 - val_loss: 4.0701 - val_accuracy: 0.4840\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.5297 - accuracy: 0.6848 - val_loss: 3.8943 - val_accuracy: 0.5018\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0758 - accuracy: 0.7524 - val_loss: 3.5233 - val_accuracy: 0.5160\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.7357 - accuracy: 0.8148 - val_loss: 3.5460 - val_accuracy: 0.5089\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5126 - accuracy: 0.8522 - val_loss: 3.3232 - val_accuracy: 0.5125\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3588 - accuracy: 0.9047 - val_loss: 3.3108 - val_accuracy: 0.5160\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2500 - accuracy: 0.9430 - val_loss: 3.1953 - val_accuracy: 0.5196\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1717 - accuracy: 0.9599 - val_loss: 3.1801 - val_accuracy: 0.5196\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1197 - accuracy: 0.9768 - val_loss: 3.1727 - val_accuracy: 0.5302\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0856 - accuracy: 0.9858 - val_loss: 3.0948 - val_accuracy: 0.5231\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0620 - accuracy: 0.9884 - val_loss: 3.0897 - val_accuracy: 0.5338\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0464 - accuracy: 0.9920 - val_loss: 3.0926 - val_accuracy: 0.5302\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0377 - accuracy: 0.9964 - val_loss: 3.0562 - val_accuracy: 0.5338\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0308 - accuracy: 0.9991 - val_loss: 3.0708 - val_accuracy: 0.5338\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0269 - accuracy: 0.9991 - val_loss: 3.0809 - val_accuracy: 0.5267\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 3.0684 - val_accuracy: 0.5231\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0208 - accuracy: 0.9991 - val_loss: 3.0769 - val_accuracy: 0.5231\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9991 - val_loss: 3.0949 - val_accuracy: 0.5231\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0169 - accuracy: 0.9991 - val_loss: 3.1029 - val_accuracy: 0.5231\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0155 - accuracy: 0.9991 - val_loss: 3.1020 - val_accuracy: 0.5231\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0144 - accuracy: 0.9991 - val_loss: 3.1013 - val_accuracy: 0.5196\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 3.1014 - val_accuracy: 0.5231\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 3.1077 - val_accuracy: 0.5267\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.1080 - val_accuracy: 0.5231\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 3.1142 - val_accuracy: 0.5267\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.1152 - val_accuracy: 0.5302\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.1132 - val_accuracy: 0.5302\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.1124 - val_accuracy: 0.5302\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.1108 - val_accuracy: 0.5302\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.1175 - val_accuracy: 0.5302\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 3.1235 - val_accuracy: 0.5302\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.1233 - val_accuracy: 0.5302\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.1162 - val_accuracy: 0.5338\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.1163 - val_accuracy: 0.5338\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.1224 - val_accuracy: 0.5374\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.1301 - val_accuracy: 0.5338\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.1237 - val_accuracy: 0.5338\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.1211 - val_accuracy: 0.5374\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.1218 - val_accuracy: 0.5374\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.1240 - val_accuracy: 0.5374\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.1250 - val_accuracy: 0.5374\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.1254 - val_accuracy: 0.5374\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.1252 - val_accuracy: 0.5409\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.1270 - val_accuracy: 0.5409\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.1278 - val_accuracy: 0.5409\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.1321 - val_accuracy: 0.5409\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.1335 - val_accuracy: 0.5409\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.1318 - val_accuracy: 0.5374\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.1303 - val_accuracy: 0.5409\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.1302 - val_accuracy: 0.5409\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.1360 - val_accuracy: 0.5374\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.1375 - val_accuracy: 0.5338\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.1375 - val_accuracy: 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 45ms/step - loss: 6.4707 - accuracy: 0.5120 - val_loss: 3.6025 - val_accuracy: 0.4911\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.0468 - accuracy: 0.5450 - val_loss: 4.5687 - val_accuracy: 0.5302\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.2390 - accuracy: 0.6002 - val_loss: 3.2609 - val_accuracy: 0.5409\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.5364 - accuracy: 0.6661 - val_loss: 2.9995 - val_accuracy: 0.5338\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.9656 - accuracy: 0.7311 - val_loss: 2.8374 - val_accuracy: 0.5267\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6399 - accuracy: 0.7872 - val_loss: 2.7342 - val_accuracy: 0.5267\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4550 - accuracy: 0.8504 - val_loss: 2.4796 - val_accuracy: 0.5338\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3112 - accuracy: 0.8923 - val_loss: 2.6587 - val_accuracy: 0.5231\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2271 - accuracy: 0.9225 - val_loss: 2.5161 - val_accuracy: 0.5231\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1485 - accuracy: 0.9555 - val_loss: 2.5069 - val_accuracy: 0.5338\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1090 - accuracy: 0.9777 - val_loss: 2.5623 - val_accuracy: 0.5338\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0786 - accuracy: 0.9858 - val_loss: 2.4885 - val_accuracy: 0.5338\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0599 - accuracy: 0.9911 - val_loss: 2.5107 - val_accuracy: 0.5302\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0469 - accuracy: 0.9947 - val_loss: 2.5433 - val_accuracy: 0.5409\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0388 - accuracy: 0.9964 - val_loss: 2.5137 - val_accuracy: 0.5409\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0331 - accuracy: 0.9991 - val_loss: 2.5087 - val_accuracy: 0.5409\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 2.5275 - val_accuracy: 0.5302\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 2.5287 - val_accuracy: 0.5267\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 2.5091 - val_accuracy: 0.5302\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 2.5102 - val_accuracy: 0.5267\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 2.5251 - val_accuracy: 0.5267\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 2.5387 - val_accuracy: 0.5374\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 2.5330 - val_accuracy: 0.5231\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 2.5299 - val_accuracy: 0.5231\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 2.5392 - val_accuracy: 0.5267\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 2.5517 - val_accuracy: 0.5267\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 2.5571 - val_accuracy: 0.5231\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 2.5501 - val_accuracy: 0.5196\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 2.5551 - val_accuracy: 0.5267\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.5639 - val_accuracy: 0.5231\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.5734 - val_accuracy: 0.5302\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 2.5785 - val_accuracy: 0.5267\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.5742 - val_accuracy: 0.5302\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 2.5818 - val_accuracy: 0.5302\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 2.5856 - val_accuracy: 0.5302\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.5841 - val_accuracy: 0.5267\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.5858 - val_accuracy: 0.5267\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.5950 - val_accuracy: 0.5267\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.6012 - val_accuracy: 0.5267\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 2.6027 - val_accuracy: 0.5267\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.6063 - val_accuracy: 0.5267\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.6010 - val_accuracy: 0.5302\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.6072 - val_accuracy: 0.5267\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.6133 - val_accuracy: 0.5267\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.6187 - val_accuracy: 0.5231\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.6210 - val_accuracy: 0.5231\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.6219 - val_accuracy: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 46ms/step - loss: 6.4364 - accuracy: 0.4987 - val_loss: 4.0002 - val_accuracy: 0.5018\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3.8096 - accuracy: 0.5485 - val_loss: 4.3418 - val_accuracy: 0.5160\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.3691 - accuracy: 0.6376 - val_loss: 3.9193 - val_accuracy: 0.5053\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.0726 - accuracy: 0.6598 - val_loss: 3.8648 - val_accuracy: 0.5196\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.5641 - accuracy: 0.7026 - val_loss: 3.5996 - val_accuracy: 0.5623\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.0042 - accuracy: 0.7720 - val_loss: 3.3054 - val_accuracy: 0.5409\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.6584 - accuracy: 0.8255 - val_loss: 3.1508 - val_accuracy: 0.5480\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4804 - accuracy: 0.8673 - val_loss: 3.1661 - val_accuracy: 0.5552\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3090 - accuracy: 0.9012 - val_loss: 3.1100 - val_accuracy: 0.5516\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2004 - accuracy: 0.9243 - val_loss: 3.1404 - val_accuracy: 0.5409\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1238 - accuracy: 0.9590 - val_loss: 3.1050 - val_accuracy: 0.5694\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0824 - accuracy: 0.9768 - val_loss: 3.0552 - val_accuracy: 0.5836\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0563 - accuracy: 0.9911 - val_loss: 3.0368 - val_accuracy: 0.5694\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0398 - accuracy: 0.9964 - val_loss: 3.0776 - val_accuracy: 0.5623\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0311 - accuracy: 0.9982 - val_loss: 3.0846 - val_accuracy: 0.5801\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0252 - accuracy: 0.9991 - val_loss: 3.0696 - val_accuracy: 0.5836\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0211 - accuracy: 0.9991 - val_loss: 3.0534 - val_accuracy: 0.5765\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0185 - accuracy: 0.9991 - val_loss: 3.0601 - val_accuracy: 0.5765\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0163 - accuracy: 0.9991 - val_loss: 3.0638 - val_accuracy: 0.5801\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 3.0670 - val_accuracy: 0.5765\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 3.0699 - val_accuracy: 0.5730\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.0720 - val_accuracy: 0.5730\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.0715 - val_accuracy: 0.5730\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.0700 - val_accuracy: 0.5730\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.0707 - val_accuracy: 0.5765\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.0728 - val_accuracy: 0.5765\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.0678 - val_accuracy: 0.5730\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.0645 - val_accuracy: 0.5730\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.0616 - val_accuracy: 0.5730\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.0627 - val_accuracy: 0.5694\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.0661 - val_accuracy: 0.5694\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.0659 - val_accuracy: 0.5658\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.0639 - val_accuracy: 0.5658\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.0616 - val_accuracy: 0.5658\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.0610 - val_accuracy: 0.5658\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.0637 - val_accuracy: 0.5658\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.0624 - val_accuracy: 0.5658\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.0606 - val_accuracy: 0.5658\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.0607 - val_accuracy: 0.5658\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.0601 - val_accuracy: 0.5658\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.0593 - val_accuracy: 0.5658\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.0597 - val_accuracy: 0.5658\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.0615 - val_accuracy: 0.5658\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.0606 - val_accuracy: 0.5658\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.0566 - val_accuracy: 0.5658\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.0596 - val_accuracy: 0.5658\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.0620 - val_accuracy: 0.5623\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.0615 - val_accuracy: 0.5658\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.0603 - val_accuracy: 0.5658\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.0593 - val_accuracy: 0.5658\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.0601 - val_accuracy: 0.5658\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.0602 - val_accuracy: 0.5658\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.0615 - val_accuracy: 0.5658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 47ms/step - loss: 5.2700 - accuracy: 0.4996 - val_loss: 4.3924 - val_accuracy: 0.4626\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.9863 - accuracy: 0.5628 - val_loss: 3.4667 - val_accuracy: 0.5125\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.2131 - accuracy: 0.6305 - val_loss: 3.6642 - val_accuracy: 0.4769\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.5128 - accuracy: 0.6768 - val_loss: 3.4600 - val_accuracy: 0.5018\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0292 - accuracy: 0.7444 - val_loss: 3.2667 - val_accuracy: 0.5053\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7420 - accuracy: 0.7890 - val_loss: 3.4137 - val_accuracy: 0.5267\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5424 - accuracy: 0.8451 - val_loss: 3.3519 - val_accuracy: 0.4982\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.3406 - accuracy: 0.8940 - val_loss: 3.3564 - val_accuracy: 0.5196\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2430 - accuracy: 0.9270 - val_loss: 3.2799 - val_accuracy: 0.5125\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1662 - accuracy: 0.9501 - val_loss: 3.2999 - val_accuracy: 0.4947\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1112 - accuracy: 0.9679 - val_loss: 3.3850 - val_accuracy: 0.4982\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0693 - accuracy: 0.9866 - val_loss: 3.3302 - val_accuracy: 0.5196\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0514 - accuracy: 0.9929 - val_loss: 3.3980 - val_accuracy: 0.5053\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0402 - accuracy: 0.9964 - val_loss: 3.3929 - val_accuracy: 0.5196\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0308 - accuracy: 0.9982 - val_loss: 3.3781 - val_accuracy: 0.5231\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0259 - accuracy: 0.9991 - val_loss: 3.3247 - val_accuracy: 0.5125\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0221 - accuracy: 0.9991 - val_loss: 3.3209 - val_accuracy: 0.5125\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0194 - accuracy: 0.9991 - val_loss: 3.3519 - val_accuracy: 0.5231\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 3.3468 - val_accuracy: 0.5267\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 3.3481 - val_accuracy: 0.5267\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 3.3409 - val_accuracy: 0.5267\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 3.3449 - val_accuracy: 0.5231\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 3.3457 - val_accuracy: 0.5231\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 3.3465 - val_accuracy: 0.5231\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.3424 - val_accuracy: 0.5231\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.3388 - val_accuracy: 0.5231\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.3351 - val_accuracy: 0.5267\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.3378 - val_accuracy: 0.5231\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 3.3379 - val_accuracy: 0.5267\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 3.3359 - val_accuracy: 0.5267\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.3252 - val_accuracy: 0.5267\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.3282 - val_accuracy: 0.5267\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.3313 - val_accuracy: 0.5267\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.3307 - val_accuracy: 0.5302\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.3280 - val_accuracy: 0.5302\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 3.3266 - val_accuracy: 0.5302\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.3219 - val_accuracy: 0.5338\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.3277 - val_accuracy: 0.5338\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.3315 - val_accuracy: 0.5338\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.3347 - val_accuracy: 0.5338\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.3338 - val_accuracy: 0.5338\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.3314 - val_accuracy: 0.5338\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.3258 - val_accuracy: 0.5338\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.3255 - val_accuracy: 0.5302\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.3307 - val_accuracy: 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 2s 46ms/step - loss: 11.6763 - accuracy: 0.4835 - val_loss: 6.2362 - val_accuracy: 0.5053\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.0985 - accuracy: 0.5245 - val_loss: 5.7559 - val_accuracy: 0.5053\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.1937 - accuracy: 0.5628 - val_loss: 4.3292 - val_accuracy: 0.5338\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.7671 - accuracy: 0.6046 - val_loss: 4.3982 - val_accuracy: 0.5302\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.9308 - accuracy: 0.6830 - val_loss: 4.1964 - val_accuracy: 0.5231\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.3780 - accuracy: 0.7551 - val_loss: 4.0607 - val_accuracy: 0.5089\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.9962 - accuracy: 0.7925 - val_loss: 3.8193 - val_accuracy: 0.5231\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.7528 - accuracy: 0.8255 - val_loss: 3.9130 - val_accuracy: 0.5053\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5178 - accuracy: 0.8638 - val_loss: 3.6886 - val_accuracy: 0.5125\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3594 - accuracy: 0.8931 - val_loss: 3.6744 - val_accuracy: 0.5125\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2461 - accuracy: 0.9234 - val_loss: 3.8310 - val_accuracy: 0.5160\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1631 - accuracy: 0.9484 - val_loss: 3.7358 - val_accuracy: 0.5231\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1007 - accuracy: 0.9733 - val_loss: 3.6957 - val_accuracy: 0.5231\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0665 - accuracy: 0.9875 - val_loss: 3.7438 - val_accuracy: 0.5302\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0477 - accuracy: 0.9955 - val_loss: 3.7491 - val_accuracy: 0.5231\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0361 - accuracy: 0.9964 - val_loss: 3.7269 - val_accuracy: 0.5338\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0279 - accuracy: 0.9973 - val_loss: 3.7408 - val_accuracy: 0.5338\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0232 - accuracy: 0.9982 - val_loss: 3.7526 - val_accuracy: 0.5267\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0203 - accuracy: 0.9982 - val_loss: 3.7515 - val_accuracy: 0.5267\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0177 - accuracy: 0.9991 - val_loss: 3.7479 - val_accuracy: 0.5231\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 3.7492 - val_accuracy: 0.5231\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.7552 - val_accuracy: 0.5196\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 3.7575 - val_accuracy: 0.5231\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 3.7607 - val_accuracy: 0.5231\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 3.7586 - val_accuracy: 0.5231\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.7596 - val_accuracy: 0.5231\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.7659 - val_accuracy: 0.5231\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.7702 - val_accuracy: 0.5267\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.7697 - val_accuracy: 0.5267\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.7644 - val_accuracy: 0.5231\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.7669 - val_accuracy: 0.5267\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.7695 - val_accuracy: 0.5267\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.7708 - val_accuracy: 0.5302\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.7705 - val_accuracy: 0.5302\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.7726 - val_accuracy: 0.5302\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.7746 - val_accuracy: 0.5302\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.7766 - val_accuracy: 0.5302\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.7791 - val_accuracy: 0.5302\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.7754 - val_accuracy: 0.5267\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.7760 - val_accuracy: 0.5267\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.7757 - val_accuracy: 0.5267\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.7787 - val_accuracy: 0.5267\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.7813 - val_accuracy: 0.5267\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.7846 - val_accuracy: 0.5267\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.7799 - val_accuracy: 0.5267\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.7791 - val_accuracy: 0.5267\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.7822 - val_accuracy: 0.5267\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.7823 - val_accuracy: 0.5267\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.7853 - val_accuracy: 0.5267\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.7862 - val_accuracy: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 45ms/step - loss: 12.4378 - accuracy: 0.5067 - val_loss: 6.4266 - val_accuracy: 0.5231\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.8224 - accuracy: 0.5361 - val_loss: 6.4063 - val_accuracy: 0.4484\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.6061 - accuracy: 0.5672 - val_loss: 5.2317 - val_accuracy: 0.4769\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.5412 - accuracy: 0.5922 - val_loss: 4.9271 - val_accuracy: 0.4733\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.7140 - accuracy: 0.6402 - val_loss: 4.7356 - val_accuracy: 0.5089\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.9414 - accuracy: 0.6812 - val_loss: 4.3451 - val_accuracy: 0.5018\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.3589 - accuracy: 0.7418 - val_loss: 4.1185 - val_accuracy: 0.5160\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.9507 - accuracy: 0.7836 - val_loss: 3.9552 - val_accuracy: 0.5231\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6115 - accuracy: 0.8424 - val_loss: 3.7146 - val_accuracy: 0.5267\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4744 - accuracy: 0.8584 - val_loss: 3.6602 - val_accuracy: 0.5160\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3025 - accuracy: 0.9020 - val_loss: 3.7468 - val_accuracy: 0.5196\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2054 - accuracy: 0.9359 - val_loss: 3.6313 - val_accuracy: 0.5053\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1479 - accuracy: 0.9555 - val_loss: 3.5269 - val_accuracy: 0.5089\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1146 - accuracy: 0.9671 - val_loss: 3.5750 - val_accuracy: 0.5125\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1013 - accuracy: 0.9760 - val_loss: 3.6112 - val_accuracy: 0.5160\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0707 - accuracy: 0.9831 - val_loss: 3.5390 - val_accuracy: 0.5089\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0439 - accuracy: 0.9902 - val_loss: 3.5165 - val_accuracy: 0.5053\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0366 - accuracy: 0.9920 - val_loss: 3.5115 - val_accuracy: 0.5053\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0279 - accuracy: 0.9964 - val_loss: 3.5202 - val_accuracy: 0.4982\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0236 - accuracy: 0.9982 - val_loss: 3.5294 - val_accuracy: 0.5018\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0204 - accuracy: 0.9991 - val_loss: 3.5408 - val_accuracy: 0.5018\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 3.5407 - val_accuracy: 0.5053\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 3.5315 - val_accuracy: 0.5053\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 3.5147 - val_accuracy: 0.5018\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 3.5147 - val_accuracy: 0.5018\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 3.5177 - val_accuracy: 0.5053\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 3.5220 - val_accuracy: 0.5053\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.5173 - val_accuracy: 0.5053\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.5134 - val_accuracy: 0.5053\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.5158 - val_accuracy: 0.5053\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.5159 - val_accuracy: 0.5053\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 3.5157 - val_accuracy: 0.5053\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.5144 - val_accuracy: 0.5053\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 3.5135 - val_accuracy: 0.5053\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.5106 - val_accuracy: 0.5053\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.5118 - val_accuracy: 0.5053\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.5122 - val_accuracy: 0.5053\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.5131 - val_accuracy: 0.5053\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.5152 - val_accuracy: 0.5089\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.5161 - val_accuracy: 0.5125\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5144 - val_accuracy: 0.5125\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.5157 - val_accuracy: 0.5125\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.5152 - val_accuracy: 0.5125\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.5150 - val_accuracy: 0.5125\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5139 - val_accuracy: 0.5125\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 236ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.5115 - val_accuracy: 0.5125\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.5113 - val_accuracy: 0.5125\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.5145 - val_accuracy: 0.5125\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.5141 - val_accuracy: 0.5125\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.5129 - val_accuracy: 0.5125\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.5117 - val_accuracy: 0.5125\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5096 - val_accuracy: 0.5125\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.5111 - val_accuracy: 0.5125\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5112 - val_accuracy: 0.5125\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.5105 - val_accuracy: 0.5125\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.5127 - val_accuracy: 0.5125\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.5130 - val_accuracy: 0.5125\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.5128 - val_accuracy: 0.5125\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.5115 - val_accuracy: 0.5125\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.5130 - val_accuracy: 0.5125\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5113 - val_accuracy: 0.5125\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.5121 - val_accuracy: 0.5125\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.5129 - val_accuracy: 0.5125\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.5133 - val_accuracy: 0.5125\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.5129 - val_accuracy: 0.5125\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.5133 - val_accuracy: 0.5160\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.5142 - val_accuracy: 0.5160\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.5165 - val_accuracy: 0.5160\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.5177 - val_accuracy: 0.5160\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.5164 - val_accuracy: 0.5160\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.5157 - val_accuracy: 0.5160\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.5147 - val_accuracy: 0.5196\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.5153 - val_accuracy: 0.5196\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.5155 - val_accuracy: 0.5196\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.5151 - val_accuracy: 0.5196\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.5154 - val_accuracy: 0.5196\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.5150 - val_accuracy: 0.5196\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5166 - val_accuracy: 0.5196\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5169 - val_accuracy: 0.5231\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5172 - val_accuracy: 0.5231\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5178 - val_accuracy: 0.5196\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.5185 - val_accuracy: 0.5196\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.5173 - val_accuracy: 0.5231\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.5170 - val_accuracy: 0.5231\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.5183 - val_accuracy: 0.5231\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.5182 - val_accuracy: 0.5231\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.5184 - val_accuracy: 0.5231\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.5196 - val_accuracy: 0.5231\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.5213 - val_accuracy: 0.5231\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.5211 - val_accuracy: 0.5231\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.5218 - val_accuracy: 0.5231\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.5206 - val_accuracy: 0.5231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vgg_mod9/assets\n"
     ]
    }
   ],
   "source": [
    "for s in range(0,10):\n",
    "    adam=tf.keras.optimizers.Adam(learning_rate=0.0001)#learning_rate=0.0005\n",
    "    vgg_mod=attention_vgg_residual_inception(input_shape, filters, num_classes)\n",
    "    vgg_mod.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) # optimizer = 'adam'\n",
    "    callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40,restore_best_weights = True)\n",
    "    history=vgg_mod.fit( X_all_shuffled[s].reshape(1404, 19, 1280,1),# \n",
    "                         to_categorical(Y_all_shuffled[s]),epochs=200, batch_size=200, validation_split=0.2, callbacks=[callback],verbose=1)   \n",
    "    os.chdir('/home/jupy/ICPR_VGG')\n",
    "    tf.keras.models.save_model(vgg_mod,filepath=f'vgg_mod{s}',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69f11291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 25ms/step\n",
      "ep score 0.5641025641025641\n",
      "23.818039 25.626497\n",
      "25.170145 25.809319\n",
      "25.691107 25.54649\n",
      "25.296503 21.367012\n",
      "23.229858 24.117794\n",
      "24.688972 24.872889\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5897435897435898\n",
      "15.877361 13.251434\n",
      "11.568411 20.966969\n",
      "8.478392 9.962566\n",
      "12.110574 15.054521\n",
      "16.629915 15.873425\n",
      "14.335865 14.591357\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "ep score 0.5448717948717948\n",
      "2.7949274 3.191945\n",
      "2.0466614 1.1973666\n",
      "3.3379679 4.2124023\n",
      "6.0490127 0.9468225\n",
      "5.9539595 4.1894817\n",
      "1.5500462 1.5971104\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.47435897435897434\n",
      "15.237129 17.256319\n",
      "17.71248 18.657547\n",
      "14.040426 11.200073\n",
      "17.021805 16.055399\n",
      "12.782481 15.698158\n",
      "9.803522 12.230139\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5\n",
      "0.9931742 1.289728\n",
      "0.018786695 0.26510155\n",
      "1.2980843 0.7630377\n",
      "2.9249277 0.63093674\n",
      "1.7277752 2.4211724\n",
      "3.1418912 1.4301621\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5384615384615384\n",
      "17.27652 21.022524\n",
      "24.073978 23.182482\n",
      "21.241808 19.573017\n",
      "22.17916 21.752502\n",
      "21.013435 19.680643\n",
      "21.238188 21.350363\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5512820512820513\n",
      "19.995909 21.13982\n",
      "22.076197 19.333347\n",
      "20.726435 23.705133\n",
      "19.391422 21.60148\n",
      "20.852516 22.593767\n",
      "21.587492 21.597027\n",
      "score 0.3333333333333333\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5897435897435898\n",
      "1.2189442 0.24651197\n",
      "2.197055 1.2269102\n",
      "4.152713 4.703607\n",
      "4.0388045 1.1270971\n",
      "4.983471 2.8270106\n",
      "2.1329858 1.3276526\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5641025641025641\n",
      "16.54125 15.774759\n",
      "19.10133 15.534092\n",
      "13.167378 16.562302\n",
      "23.016127 17.631287\n",
      "19.87508 13.456353\n",
      "17.068369 15.347632\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.4807692307692308\n",
      "1.8009794 3.0036492\n",
      "4.394773 3.510303\n",
      "1.7613915 5.7097206\n",
      "0.1139471 3.0096707\n",
      "3.547335 3.0660608\n",
      "4.8371954 7.3258986\n",
      "score 0.5\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/jupy/ICPR_VGG')\n",
    "avg_score=0\n",
    "for s in range(0,10):\n",
    "    classifier=load_model(f'vgg_mod{s}')\n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "    avg_score=avg_score+accuracy_score(pred,y_test_short)\n",
    "    print('score',accuracy_score(pred,y_test_short))\n",
    "print(avg_score/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986d2f0",
   "metadata": {},
   "source": [
    "# CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a812761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_46 (InputLayer)       [(None, 19, 1280)]        0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 19, 1280, 1)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 17, 1278, 32)      320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 15, 1276, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 7, 638, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 7, 40832)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               20972032  \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,991,106\n",
      "Trainable params: 20,991,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_lstm_model(input_shape):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Reshape the input to (time steps, features)\n",
    "    reshaped_input = Reshape((input_shape[0], input_shape[1], 1))(input_layer)\n",
    "    \n",
    "    # CNN layers\n",
    "    cnn_conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(reshaped_input)\n",
    "    cnn_conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(cnn_conv1)\n",
    "    cnn_maxpool = MaxPooling2D(pool_size=(2, 2))(cnn_conv2)\n",
    "    \n",
    "    # Reshape the output of CNN layers to (time steps, features)\n",
    "    cnn_output = Reshape((-1, cnn_maxpool.shape[2] * cnn_maxpool.shape[3]))(cnn_maxpool)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm = LSTM(128)(cnn_output)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(2, activation='sigmoid')(lstm)\n",
    "    \n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (19, 1280)\n",
    "cnn_lstm = cnn_lstm_model(input_shape)\n",
    "cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ed1121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 48ms/step - loss: 6.3747 - accuracy: 0.4978 - val_loss: 5.0755 - val_accuracy: 0.5160\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.2471 - accuracy: 0.5619 - val_loss: 4.7924 - val_accuracy: 0.5445\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.6673 - accuracy: 0.6242 - val_loss: 4.6285 - val_accuracy: 0.5302\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.1398 - accuracy: 0.6483 - val_loss: 4.7003 - val_accuracy: 0.5196\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4728 - accuracy: 0.7035 - val_loss: 3.8036 - val_accuracy: 0.5694\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.0150 - accuracy: 0.7729 - val_loss: 4.1280 - val_accuracy: 0.5231\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6678 - accuracy: 0.8121 - val_loss: 3.7495 - val_accuracy: 0.5267\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4032 - accuracy: 0.8709 - val_loss: 3.7784 - val_accuracy: 0.5231\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2721 - accuracy: 0.9136 - val_loss: 3.6190 - val_accuracy: 0.5196\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1778 - accuracy: 0.9546 - val_loss: 3.7309 - val_accuracy: 0.5196\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1153 - accuracy: 0.9724 - val_loss: 3.6786 - val_accuracy: 0.5089\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0765 - accuracy: 0.9840 - val_loss: 3.7374 - val_accuracy: 0.5125\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0551 - accuracy: 0.9849 - val_loss: 3.8981 - val_accuracy: 0.4947\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0410 - accuracy: 0.9938 - val_loss: 3.7068 - val_accuracy: 0.4947\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0306 - accuracy: 0.9973 - val_loss: 3.7677 - val_accuracy: 0.4911\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0245 - accuracy: 0.9982 - val_loss: 3.7764 - val_accuracy: 0.4982\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0193 - accuracy: 0.9991 - val_loss: 3.7185 - val_accuracy: 0.4947\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 3.7050 - val_accuracy: 0.5018\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 3.7349 - val_accuracy: 0.4982\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 3.7552 - val_accuracy: 0.4982\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 3.7505 - val_accuracy: 0.5018\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.7385 - val_accuracy: 0.5018\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 3.7411 - val_accuracy: 0.5018\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 3.7513 - val_accuracy: 0.5018\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.7556 - val_accuracy: 0.5018\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.7538 - val_accuracy: 0.5018\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.7579 - val_accuracy: 0.5018\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.7617 - val_accuracy: 0.5018\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.7629 - val_accuracy: 0.4982\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.7635 - val_accuracy: 0.4982\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.7676 - val_accuracy: 0.5018\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 3.7687 - val_accuracy: 0.5018\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.7706 - val_accuracy: 0.5018\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.7711 - val_accuracy: 0.5018\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.7701 - val_accuracy: 0.5053\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.7755 - val_accuracy: 0.5053\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.7780 - val_accuracy: 0.5053\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.7798 - val_accuracy: 0.5053\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.7754 - val_accuracy: 0.5089\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.7745 - val_accuracy: 0.5089\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.7784 - val_accuracy: 0.5089\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.7803 - val_accuracy: 0.5089\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.7792 - val_accuracy: 0.5089\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.7806 - val_accuracy: 0.5089\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.7824 - val_accuracy: 0.5089\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.7820 - val_accuracy: 0.5089\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.7849 - val_accuracy: 0.5089\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.7910 - val_accuracy: 0.5089\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.7896 - val_accuracy: 0.5125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 45ms/step - loss: 8.1531 - accuracy: 0.4826 - val_loss: 5.9345 - val_accuracy: 0.4484\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.6724 - accuracy: 0.5352 - val_loss: 5.7541 - val_accuracy: 0.4555\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.4009 - accuracy: 0.5592 - val_loss: 4.0923 - val_accuracy: 0.4840\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.4598 - accuracy: 0.6242 - val_loss: 3.7694 - val_accuracy: 0.5231\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.7438 - accuracy: 0.6830 - val_loss: 3.6907 - val_accuracy: 0.5125\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.2708 - accuracy: 0.7231 - val_loss: 3.3140 - val_accuracy: 0.5267\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.8958 - accuracy: 0.7747 - val_loss: 3.3590 - val_accuracy: 0.5125\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6459 - accuracy: 0.8175 - val_loss: 3.2665 - val_accuracy: 0.5231\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4401 - accuracy: 0.8611 - val_loss: 3.1297 - val_accuracy: 0.5231\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3056 - accuracy: 0.8976 - val_loss: 3.1525 - val_accuracy: 0.5231\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1950 - accuracy: 0.9439 - val_loss: 2.9947 - val_accuracy: 0.5338\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1271 - accuracy: 0.9644 - val_loss: 2.9788 - val_accuracy: 0.5409\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0875 - accuracy: 0.9760 - val_loss: 2.9741 - val_accuracy: 0.5338\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0616 - accuracy: 0.9902 - val_loss: 2.9905 - val_accuracy: 0.5338\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0450 - accuracy: 0.9947 - val_loss: 2.9665 - val_accuracy: 0.5302\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0356 - accuracy: 0.9982 - val_loss: 2.9685 - val_accuracy: 0.5267\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0300 - accuracy: 0.9991 - val_loss: 2.9600 - val_accuracy: 0.5338\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0258 - accuracy: 0.9991 - val_loss: 2.9667 - val_accuracy: 0.5409\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 2.9751 - val_accuracy: 0.5409\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 2.9741 - val_accuracy: 0.5374\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 2.9752 - val_accuracy: 0.5374\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 2.9752 - val_accuracy: 0.5374\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.9800 - val_accuracy: 0.5374\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 2.9868 - val_accuracy: 0.5445\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 2.9882 - val_accuracy: 0.5409\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 2.9854 - val_accuracy: 0.5374\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.9845 - val_accuracy: 0.5445\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.9881 - val_accuracy: 0.5445\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 2.9926 - val_accuracy: 0.5445\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.9915 - val_accuracy: 0.5374\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.9938 - val_accuracy: 0.5445\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.9990 - val_accuracy: 0.5480\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.0017 - val_accuracy: 0.5480\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.0010 - val_accuracy: 0.5516\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.9989 - val_accuracy: 0.5516\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.0039 - val_accuracy: 0.5516\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.0081 - val_accuracy: 0.5516\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.0092 - val_accuracy: 0.5516\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.0074 - val_accuracy: 0.5516\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.0101 - val_accuracy: 0.5516\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.0103 - val_accuracy: 0.5516\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.0083 - val_accuracy: 0.5480\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.0082 - val_accuracy: 0.5480\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.0100 - val_accuracy: 0.5480\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.0144 - val_accuracy: 0.5516\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.0177 - val_accuracy: 0.5480\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.0152 - val_accuracy: 0.5480\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.0140 - val_accuracy: 0.5516\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.0173 - val_accuracy: 0.5516\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.0194 - val_accuracy: 0.5516\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.0204 - val_accuracy: 0.5516\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.0203 - val_accuracy: 0.5516\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.0190 - val_accuracy: 0.5516\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.0194 - val_accuracy: 0.5516\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.0218 - val_accuracy: 0.5552\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.0220 - val_accuracy: 0.5552\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.0227 - val_accuracy: 0.5587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 46ms/step - loss: 11.6490 - accuracy: 0.5156 - val_loss: 6.5014 - val_accuracy: 0.4911\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.2043 - accuracy: 0.5191 - val_loss: 6.0303 - val_accuracy: 0.4840\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.1163 - accuracy: 0.5726 - val_loss: 4.8293 - val_accuracy: 0.5018\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.7973 - accuracy: 0.6305 - val_loss: 4.2802 - val_accuracy: 0.5196\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.0429 - accuracy: 0.6723 - val_loss: 3.7008 - val_accuracy: 0.5160\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.2610 - accuracy: 0.7320 - val_loss: 4.0007 - val_accuracy: 0.5125\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.9223 - accuracy: 0.7649 - val_loss: 3.6550 - val_accuracy: 0.5053\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6199 - accuracy: 0.8210 - val_loss: 3.5386 - val_accuracy: 0.5267\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4007 - accuracy: 0.8655 - val_loss: 3.5038 - val_accuracy: 0.5196\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2776 - accuracy: 0.8994 - val_loss: 3.3141 - val_accuracy: 0.5409\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1847 - accuracy: 0.9359 - val_loss: 3.6536 - val_accuracy: 0.5160\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1203 - accuracy: 0.9662 - val_loss: 3.4844 - val_accuracy: 0.5089\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0798 - accuracy: 0.9813 - val_loss: 3.5398 - val_accuracy: 0.5089\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0592 - accuracy: 0.9858 - val_loss: 3.5763 - val_accuracy: 0.4875\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0410 - accuracy: 0.9947 - val_loss: 3.4396 - val_accuracy: 0.5018\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0326 - accuracy: 0.9964 - val_loss: 3.4807 - val_accuracy: 0.4911\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0249 - accuracy: 0.9982 - val_loss: 3.5459 - val_accuracy: 0.4840\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 0.9991 - val_loss: 3.5343 - val_accuracy: 0.4911\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 3.4914 - val_accuracy: 0.4947\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 3.4810 - val_accuracy: 0.4911\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 3.4908 - val_accuracy: 0.4875\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 3.5075 - val_accuracy: 0.4947\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 3.5068 - val_accuracy: 0.4875\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 3.4968 - val_accuracy: 0.4911\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.4977 - val_accuracy: 0.4947\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.5013 - val_accuracy: 0.4911\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 3.5075 - val_accuracy: 0.4911\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.5007 - val_accuracy: 0.4911\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.4975 - val_accuracy: 0.4911\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.5071 - val_accuracy: 0.4911\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.5118 - val_accuracy: 0.4911\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.5075 - val_accuracy: 0.4911\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.5025 - val_accuracy: 0.4911\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.5001 - val_accuracy: 0.4911\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.5083 - val_accuracy: 0.4911\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.5106 - val_accuracy: 0.4911\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.5101 - val_accuracy: 0.4947\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.5045 - val_accuracy: 0.4947\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.5027 - val_accuracy: 0.4947\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.5076 - val_accuracy: 0.4947\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.5108 - val_accuracy: 0.4911\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.5066 - val_accuracy: 0.4947\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 3.5086 - val_accuracy: 0.4911\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.5078 - val_accuracy: 0.4947\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.5105 - val_accuracy: 0.4911\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.5164 - val_accuracy: 0.4947\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.5153 - val_accuracy: 0.4947\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.5117 - val_accuracy: 0.4947\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5093 - val_accuracy: 0.4947\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.5130 - val_accuracy: 0.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 5.7632 - accuracy: 0.4684 - val_loss: 4.8427 - val_accuracy: 0.4840\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.9421 - accuracy: 0.5459 - val_loss: 4.0775 - val_accuracy: 0.5267\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.7697 - accuracy: 0.6064 - val_loss: 3.9232 - val_accuracy: 0.5196\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.8353 - accuracy: 0.6723 - val_loss: 3.5953 - val_accuracy: 0.5302\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.4709 - accuracy: 0.7186 - val_loss: 3.5412 - val_accuracy: 0.5445\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.0749 - accuracy: 0.7524 - val_loss: 3.3090 - val_accuracy: 0.5730\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.7239 - accuracy: 0.8077 - val_loss: 3.0708 - val_accuracy: 0.5658\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5008 - accuracy: 0.8344 - val_loss: 3.2081 - val_accuracy: 0.5552\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3406 - accuracy: 0.8905 - val_loss: 3.1797 - val_accuracy: 0.5623\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2446 - accuracy: 0.9216 - val_loss: 3.0815 - val_accuracy: 0.5623\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1736 - accuracy: 0.9475 - val_loss: 2.9737 - val_accuracy: 0.5694\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1140 - accuracy: 0.9662 - val_loss: 3.0649 - val_accuracy: 0.5623\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0745 - accuracy: 0.9831 - val_loss: 2.9688 - val_accuracy: 0.5730\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0546 - accuracy: 0.9929 - val_loss: 2.9864 - val_accuracy: 0.5694\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0400 - accuracy: 0.9964 - val_loss: 3.0243 - val_accuracy: 0.5658\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0325 - accuracy: 0.9964 - val_loss: 2.9932 - val_accuracy: 0.5730\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0268 - accuracy: 0.9973 - val_loss: 2.9783 - val_accuracy: 0.5730\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0230 - accuracy: 0.9991 - val_loss: 3.0000 - val_accuracy: 0.5765\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 2.9823 - val_accuracy: 0.5765\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 2.9742 - val_accuracy: 0.5765\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 2.9841 - val_accuracy: 0.5801\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 2.9778 - val_accuracy: 0.5801\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 2.9703 - val_accuracy: 0.5801\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 2.9726 - val_accuracy: 0.5765\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 2.9732 - val_accuracy: 0.5801\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.9766 - val_accuracy: 0.5801\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 2.9773 - val_accuracy: 0.5872\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.9734 - val_accuracy: 0.5872\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.9710 - val_accuracy: 0.5872\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.9772 - val_accuracy: 0.5872\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.9807 - val_accuracy: 0.5872\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.9765 - val_accuracy: 0.5872\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.9731 - val_accuracy: 0.5872\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.9786 - val_accuracy: 0.5872\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.9784 - val_accuracy: 0.5907\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.9752 - val_accuracy: 0.5907\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.9796 - val_accuracy: 0.5907\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.9755 - val_accuracy: 0.5907\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.9763 - val_accuracy: 0.5907\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.9750 - val_accuracy: 0.5907\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.9829 - val_accuracy: 0.5872\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.9781 - val_accuracy: 0.5872\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.9784 - val_accuracy: 0.5872\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.9790 - val_accuracy: 0.5872\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.9829 - val_accuracy: 0.5872\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.9821 - val_accuracy: 0.5872\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.9813 - val_accuracy: 0.5872\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.9805 - val_accuracy: 0.5836\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.9826 - val_accuracy: 0.5836\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.9851 - val_accuracy: 0.5801\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.9854 - val_accuracy: 0.5801\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.9862 - val_accuracy: 0.5801\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.9862 - val_accuracy: 0.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 45ms/step - loss: 7.5277 - accuracy: 0.5174 - val_loss: 5.1413 - val_accuracy: 0.5196\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.8716 - accuracy: 0.5423 - val_loss: 4.6628 - val_accuracy: 0.4911\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.3243 - accuracy: 0.6037 - val_loss: 4.4025 - val_accuracy: 0.5374\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.4288 - accuracy: 0.6456 - val_loss: 3.7996 - val_accuracy: 0.5445\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.7949 - accuracy: 0.6910 - val_loss: 3.9117 - val_accuracy: 0.5409\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4698 - accuracy: 0.7240 - val_loss: 3.5545 - val_accuracy: 0.5374\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.0712 - accuracy: 0.7720 - val_loss: 3.5785 - val_accuracy: 0.5338\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7553 - accuracy: 0.8050 - val_loss: 3.3411 - val_accuracy: 0.5587\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5872 - accuracy: 0.8379 - val_loss: 3.3213 - val_accuracy: 0.5587\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3971 - accuracy: 0.8664 - val_loss: 3.2812 - val_accuracy: 0.5516\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2701 - accuracy: 0.9110 - val_loss: 3.2575 - val_accuracy: 0.5587\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1746 - accuracy: 0.9466 - val_loss: 3.2623 - val_accuracy: 0.5480\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1199 - accuracy: 0.9671 - val_loss: 3.1377 - val_accuracy: 0.5801\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0927 - accuracy: 0.9768 - val_loss: 3.2702 - val_accuracy: 0.5730\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0928 - accuracy: 0.9786 - val_loss: 3.2211 - val_accuracy: 0.5765\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0620 - accuracy: 0.9875 - val_loss: 3.2115 - val_accuracy: 0.5694\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0430 - accuracy: 0.9947 - val_loss: 3.2735 - val_accuracy: 0.5694\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0351 - accuracy: 0.9947 - val_loss: 3.2157 - val_accuracy: 0.5694\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0283 - accuracy: 0.9973 - val_loss: 3.1744 - val_accuracy: 0.5801\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0226 - accuracy: 0.9991 - val_loss: 3.1984 - val_accuracy: 0.5730\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0193 - accuracy: 0.9991 - val_loss: 3.2324 - val_accuracy: 0.5765\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 3.2223 - val_accuracy: 0.5694\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.2106 - val_accuracy: 0.5658\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 3.2128 - val_accuracy: 0.5658\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 3.2223 - val_accuracy: 0.5658\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 3.2292 - val_accuracy: 0.5658\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.2302 - val_accuracy: 0.5658\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 3.2354 - val_accuracy: 0.5658\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.2424 - val_accuracy: 0.5658\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.2431 - val_accuracy: 0.5694\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.2383 - val_accuracy: 0.5658\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 3.2416 - val_accuracy: 0.5658\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.2449 - val_accuracy: 0.5694\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.2487 - val_accuracy: 0.5694\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.2469 - val_accuracy: 0.5658\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.2471 - val_accuracy: 0.5694\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.2548 - val_accuracy: 0.5694\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.2551 - val_accuracy: 0.5658\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 3.2563 - val_accuracy: 0.5694\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.2580 - val_accuracy: 0.5658\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.2619 - val_accuracy: 0.5658\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.2597 - val_accuracy: 0.5658\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.2624 - val_accuracy: 0.5694\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.2633 - val_accuracy: 0.5694\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.2639 - val_accuracy: 0.5658\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.2677 - val_accuracy: 0.5694\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.2699 - val_accuracy: 0.5730\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.2702 - val_accuracy: 0.5730\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.2708 - val_accuracy: 0.5730\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.2745 - val_accuracy: 0.5730\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.2733 - val_accuracy: 0.5730\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.2752 - val_accuracy: 0.5730\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.2768 - val_accuracy: 0.5730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 45ms/step - loss: 5.6344 - accuracy: 0.5174 - val_loss: 4.2162 - val_accuracy: 0.5267\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.3273 - accuracy: 0.5815 - val_loss: 4.0014 - val_accuracy: 0.5445\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.3728 - accuracy: 0.6037 - val_loss: 4.0232 - val_accuracy: 0.5658\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.8317 - accuracy: 0.6483 - val_loss: 3.4186 - val_accuracy: 0.5338\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.1521 - accuracy: 0.7106 - val_loss: 3.4001 - val_accuracy: 0.5552\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7855 - accuracy: 0.7703 - val_loss: 2.9907 - val_accuracy: 0.5552\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.4968 - accuracy: 0.8379 - val_loss: 3.0263 - val_accuracy: 0.5694\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3247 - accuracy: 0.8789 - val_loss: 2.8347 - val_accuracy: 0.5623\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2128 - accuracy: 0.9270 - val_loss: 2.8315 - val_accuracy: 0.5587\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1315 - accuracy: 0.9635 - val_loss: 2.8629 - val_accuracy: 0.5658\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0930 - accuracy: 0.9786 - val_loss: 2.8397 - val_accuracy: 0.5623\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0657 - accuracy: 0.9866 - val_loss: 2.8089 - val_accuracy: 0.5552\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0489 - accuracy: 0.9920 - val_loss: 2.8308 - val_accuracy: 0.5658\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0370 - accuracy: 0.9955 - val_loss: 2.8569 - val_accuracy: 0.5801\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0306 - accuracy: 0.9973 - val_loss: 2.8493 - val_accuracy: 0.5658\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0252 - accuracy: 0.9991 - val_loss: 2.8660 - val_accuracy: 0.5694\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0225 - accuracy: 0.9991 - val_loss: 2.8574 - val_accuracy: 0.5801\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0205 - accuracy: 0.9991 - val_loss: 2.8827 - val_accuracy: 0.5730\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0184 - accuracy: 0.9991 - val_loss: 2.8911 - val_accuracy: 0.5765\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 2.8699 - val_accuracy: 0.5836\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 2.8733 - val_accuracy: 0.5801\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 2.8862 - val_accuracy: 0.5765\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 2.8907 - val_accuracy: 0.5765\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.8868 - val_accuracy: 0.5730\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.8889 - val_accuracy: 0.5765\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 2.9035 - val_accuracy: 0.5801\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.9101 - val_accuracy: 0.5801\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.9043 - val_accuracy: 0.5765\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.9052 - val_accuracy: 0.5730\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.9095 - val_accuracy: 0.5730\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.9188 - val_accuracy: 0.5765\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.9197 - val_accuracy: 0.5765\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.9163 - val_accuracy: 0.5730\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.9222 - val_accuracy: 0.5730\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.9259 - val_accuracy: 0.5730\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.9276 - val_accuracy: 0.5765\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.9254 - val_accuracy: 0.5694\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.9308 - val_accuracy: 0.5836\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.9314 - val_accuracy: 0.5836\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.9330 - val_accuracy: 0.5801\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.9319 - val_accuracy: 0.5765\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.9379 - val_accuracy: 0.5801\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.9407 - val_accuracy: 0.5801\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.9456 - val_accuracy: 0.5801\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.9465 - val_accuracy: 0.5801\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.9489 - val_accuracy: 0.5801\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.9481 - val_accuracy: 0.5801\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.9505 - val_accuracy: 0.5801\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.9553 - val_accuracy: 0.5836\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.9537 - val_accuracy: 0.5801\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.9580 - val_accuracy: 0.5765\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.9590 - val_accuracy: 0.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 52ms/step - loss: 4.4256 - accuracy: 0.5111 - val_loss: 3.7339 - val_accuracy: 0.5160\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.7187 - accuracy: 0.6162 - val_loss: 3.7234 - val_accuracy: 0.5053\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.0729 - accuracy: 0.6278 - val_loss: 3.1750 - val_accuracy: 0.5125\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.7073 - accuracy: 0.6687 - val_loss: 3.0723 - val_accuracy: 0.4804\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.1114 - accuracy: 0.7435 - val_loss: 3.2924 - val_accuracy: 0.4911\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.8166 - accuracy: 0.7854 - val_loss: 2.8016 - val_accuracy: 0.5231\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5514 - accuracy: 0.8308 - val_loss: 2.8112 - val_accuracy: 0.5338\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3770 - accuracy: 0.8655 - val_loss: 2.7877 - val_accuracy: 0.5338\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2370 - accuracy: 0.9145 - val_loss: 2.7815 - val_accuracy: 0.5374\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.1510 - accuracy: 0.9492 - val_loss: 2.7636 - val_accuracy: 0.5480\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1016 - accuracy: 0.9679 - val_loss: 2.7287 - val_accuracy: 0.5374\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0706 - accuracy: 0.9858 - val_loss: 2.7390 - val_accuracy: 0.5480\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0537 - accuracy: 0.9929 - val_loss: 2.7222 - val_accuracy: 0.5516\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0406 - accuracy: 0.9973 - val_loss: 2.7069 - val_accuracy: 0.5552\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0332 - accuracy: 0.9982 - val_loss: 2.7070 - val_accuracy: 0.5480\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0271 - accuracy: 0.9991 - val_loss: 2.7311 - val_accuracy: 0.5552\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0242 - accuracy: 0.9991 - val_loss: 2.7286 - val_accuracy: 0.5623\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0210 - accuracy: 0.9991 - val_loss: 2.7219 - val_accuracy: 0.5623\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0190 - accuracy: 0.9991 - val_loss: 2.7231 - val_accuracy: 0.5658\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 2.7344 - val_accuracy: 0.5658\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0157 - accuracy: 0.9991 - val_loss: 2.7394 - val_accuracy: 0.5623\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 2.7420 - val_accuracy: 0.5587\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 2.7469 - val_accuracy: 0.5587\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 2.7479 - val_accuracy: 0.5552\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 2.7522 - val_accuracy: 0.5552\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.7574 - val_accuracy: 0.5552\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.7568 - val_accuracy: 0.5552\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.7610 - val_accuracy: 0.5552\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.7622 - val_accuracy: 0.5587\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.7612 - val_accuracy: 0.5587\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.7661 - val_accuracy: 0.5587\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.7693 - val_accuracy: 0.5587\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.7751 - val_accuracy: 0.5587\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.7781 - val_accuracy: 0.5587\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.7803 - val_accuracy: 0.5587\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.7794 - val_accuracy: 0.5587\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.7845 - val_accuracy: 0.5587\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.7879 - val_accuracy: 0.5587\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.7896 - val_accuracy: 0.5587\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.7933 - val_accuracy: 0.5587\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.7938 - val_accuracy: 0.5552\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.7980 - val_accuracy: 0.5552\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.7993 - val_accuracy: 0.5552\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.8027 - val_accuracy: 0.5552\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.8064 - val_accuracy: 0.5552\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.8069 - val_accuracy: 0.5552\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.8083 - val_accuracy: 0.5552\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.8105 - val_accuracy: 0.5552\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.8139 - val_accuracy: 0.5552\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.8161 - val_accuracy: 0.5552\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.8171 - val_accuracy: 0.5552\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.8177 - val_accuracy: 0.5552\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.8199 - val_accuracy: 0.5552\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.8209 - val_accuracy: 0.5552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 46ms/step - loss: 6.6421 - accuracy: 0.5013 - val_loss: 5.2796 - val_accuracy: 0.4733\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.6382 - accuracy: 0.5574 - val_loss: 3.8113 - val_accuracy: 0.5231\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 2.3375 - accuracy: 0.6198 - val_loss: 4.0309 - val_accuracy: 0.5018\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.6156 - accuracy: 0.7026 - val_loss: 3.3444 - val_accuracy: 0.5302\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.0991 - accuracy: 0.7507 - val_loss: 3.3378 - val_accuracy: 0.5089\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.7875 - accuracy: 0.7925 - val_loss: 3.1541 - val_accuracy: 0.5160\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5221 - accuracy: 0.8468 - val_loss: 3.1947 - val_accuracy: 0.5018\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3867 - accuracy: 0.8762 - val_loss: 3.1386 - val_accuracy: 0.5160\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2756 - accuracy: 0.9136 - val_loss: 3.1175 - val_accuracy: 0.5053\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1881 - accuracy: 0.9332 - val_loss: 3.0278 - val_accuracy: 0.5409\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1253 - accuracy: 0.9510 - val_loss: 3.0675 - val_accuracy: 0.5267\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2854 - accuracy: 0.9332 - val_loss: 3.5701 - val_accuracy: 0.5374\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.6241 - accuracy: 0.8682 - val_loss: 3.8486 - val_accuracy: 0.5018\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3688 - accuracy: 0.9101 - val_loss: 3.5534 - val_accuracy: 0.5374\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2248 - accuracy: 0.9279 - val_loss: 3.5328 - val_accuracy: 0.5231\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1431 - accuracy: 0.9519 - val_loss: 3.2376 - val_accuracy: 0.5338\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0816 - accuracy: 0.9777 - val_loss: 3.1988 - val_accuracy: 0.5338\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0478 - accuracy: 0.9929 - val_loss: 3.2528 - val_accuracy: 0.5160\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0326 - accuracy: 0.9991 - val_loss: 3.1175 - val_accuracy: 0.5374\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0280 - accuracy: 0.9991 - val_loss: 3.1058 - val_accuracy: 0.5338\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 3.1373 - val_accuracy: 0.5409\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 3.1515 - val_accuracy: 0.5409\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.1366 - val_accuracy: 0.5409\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.1226 - val_accuracy: 0.5445\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 3.1210 - val_accuracy: 0.5409\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 3.1262 - val_accuracy: 0.5374\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.1328 - val_accuracy: 0.5374\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.1380 - val_accuracy: 0.5338\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 3.1380 - val_accuracy: 0.5374\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.1353 - val_accuracy: 0.5374\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.1353 - val_accuracy: 0.5374\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 3.1350 - val_accuracy: 0.5374\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.1358 - val_accuracy: 0.5374\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.1391 - val_accuracy: 0.5338\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 3.1426 - val_accuracy: 0.5338\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.1405 - val_accuracy: 0.5338\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.1436 - val_accuracy: 0.5338\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.1443 - val_accuracy: 0.5338\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.1443 - val_accuracy: 0.5338\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.1477 - val_accuracy: 0.5338\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.1471 - val_accuracy: 0.5338\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.1471 - val_accuracy: 0.5338\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.1461 - val_accuracy: 0.5338\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.1477 - val_accuracy: 0.5338\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.1496 - val_accuracy: 0.5338\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.1515 - val_accuracy: 0.5338\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.1487 - val_accuracy: 0.5338\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 3.1484 - val_accuracy: 0.5338\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.1515 - val_accuracy: 0.5338\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.1529 - val_accuracy: 0.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 48ms/step - loss: 17.1577 - accuracy: 0.4987 - val_loss: 6.6405 - val_accuracy: 0.5053\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 6.8685 - accuracy: 0.5441 - val_loss: 7.8022 - val_accuracy: 0.4626\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.6423 - accuracy: 0.5539 - val_loss: 6.3146 - val_accuracy: 0.4377\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.7919 - accuracy: 0.6037 - val_loss: 4.9791 - val_accuracy: 0.4591\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.8501 - accuracy: 0.6251 - val_loss: 4.6978 - val_accuracy: 0.4626\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.9468 - accuracy: 0.6857 - val_loss: 4.6657 - val_accuracy: 0.4555\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4199 - accuracy: 0.7168 - val_loss: 4.4324 - val_accuracy: 0.4840\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.0142 - accuracy: 0.7720 - val_loss: 4.4748 - val_accuracy: 0.4840\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7412 - accuracy: 0.8264 - val_loss: 4.3259 - val_accuracy: 0.4804\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.5296 - accuracy: 0.8655 - val_loss: 4.3840 - val_accuracy: 0.4875\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3775 - accuracy: 0.9029 - val_loss: 4.2787 - val_accuracy: 0.4911\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2783 - accuracy: 0.9252 - val_loss: 4.2499 - val_accuracy: 0.4911\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2022 - accuracy: 0.9394 - val_loss: 4.2266 - val_accuracy: 0.4947\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1444 - accuracy: 0.9608 - val_loss: 4.1735 - val_accuracy: 0.4875\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.1011 - accuracy: 0.9724 - val_loss: 4.2256 - val_accuracy: 0.4769\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0738 - accuracy: 0.9840 - val_loss: 4.2551 - val_accuracy: 0.4804\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0545 - accuracy: 0.9911 - val_loss: 4.2545 - val_accuracy: 0.4875\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0435 - accuracy: 0.9947 - val_loss: 4.2542 - val_accuracy: 0.4911\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0349 - accuracy: 0.9964 - val_loss: 4.2540 - val_accuracy: 0.4875\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0290 - accuracy: 0.9982 - val_loss: 4.2607 - val_accuracy: 0.4875\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0251 - accuracy: 0.9991 - val_loss: 4.2611 - val_accuracy: 0.4911\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 4.2535 - val_accuracy: 0.4875\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 4.2465 - val_accuracy: 0.4875\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 4.2400 - val_accuracy: 0.4875\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 4.2476 - val_accuracy: 0.4875\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 4.2592 - val_accuracy: 0.4875\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 4.2599 - val_accuracy: 0.4875\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 4.2606 - val_accuracy: 0.4911\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 4.2544 - val_accuracy: 0.4875\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 4.2569 - val_accuracy: 0.4911\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 4.2608 - val_accuracy: 0.4911\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 4.2598 - val_accuracy: 0.4911\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 4.2595 - val_accuracy: 0.4911\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 4.2630 - val_accuracy: 0.4911\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 4.2627 - val_accuracy: 0.4911\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 4.2676 - val_accuracy: 0.4911\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 4.2656 - val_accuracy: 0.4911\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 4.2625 - val_accuracy: 0.4911\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 4.2617 - val_accuracy: 0.4911\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 4.2692 - val_accuracy: 0.4911\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 4.2743 - val_accuracy: 0.4911\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 4.2766 - val_accuracy: 0.4911\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 4.2768 - val_accuracy: 0.4911\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 4.2785 - val_accuracy: 0.4911\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 4.2794 - val_accuracy: 0.4911\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 4.2799 - val_accuracy: 0.4947\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 4.2811 - val_accuracy: 0.4911\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 4.2834 - val_accuracy: 0.4947\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 4.2879 - val_accuracy: 0.4947\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.2867 - val_accuracy: 0.4947\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 4.2882 - val_accuracy: 0.4947\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 4.2872 - val_accuracy: 0.4947\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 4.2867 - val_accuracy: 0.4947\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 4.2893 - val_accuracy: 0.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 44ms/step - loss: 8.8049 - accuracy: 0.5049 - val_loss: 5.4135 - val_accuracy: 0.4698\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4.6548 - accuracy: 0.5325 - val_loss: 4.6100 - val_accuracy: 0.4591\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 3.0071 - accuracy: 0.5922 - val_loss: 4.0920 - val_accuracy: 0.5160\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2.2657 - accuracy: 0.6358 - val_loss: 4.0513 - val_accuracy: 0.4947\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.5746 - accuracy: 0.6803 - val_loss: 3.9661 - val_accuracy: 0.5125\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0607 - accuracy: 0.7551 - val_loss: 3.5711 - val_accuracy: 0.4947\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.7893 - accuracy: 0.7952 - val_loss: 3.4617 - val_accuracy: 0.5053\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5515 - accuracy: 0.8344 - val_loss: 3.4432 - val_accuracy: 0.5160\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3924 - accuracy: 0.8780 - val_loss: 3.3684 - val_accuracy: 0.5267\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2798 - accuracy: 0.9101 - val_loss: 3.3087 - val_accuracy: 0.5125\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2021 - accuracy: 0.9386 - val_loss: 3.2841 - val_accuracy: 0.5196\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1421 - accuracy: 0.9599 - val_loss: 3.2639 - val_accuracy: 0.5018\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1022 - accuracy: 0.9742 - val_loss: 3.2743 - val_accuracy: 0.5089\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0744 - accuracy: 0.9858 - val_loss: 3.2691 - val_accuracy: 0.5089\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0582 - accuracy: 0.9911 - val_loss: 3.2794 - val_accuracy: 0.5231\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0452 - accuracy: 0.9938 - val_loss: 3.2930 - val_accuracy: 0.5089\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0372 - accuracy: 0.9964 - val_loss: 3.2737 - val_accuracy: 0.5125\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0309 - accuracy: 0.9991 - val_loss: 3.2444 - val_accuracy: 0.5196\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 3.2410 - val_accuracy: 0.5196\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 3.2437 - val_accuracy: 0.5089\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 3.2456 - val_accuracy: 0.5160\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 3.2446 - val_accuracy: 0.5160\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.2369 - val_accuracy: 0.5125\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 3.2320 - val_accuracy: 0.5160\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.2394 - val_accuracy: 0.5125\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 3.2375 - val_accuracy: 0.5125\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.2333 - val_accuracy: 0.5196\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 3.2261 - val_accuracy: 0.5231\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.2307 - val_accuracy: 0.5196\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 3.2360 - val_accuracy: 0.5160\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 3.2355 - val_accuracy: 0.5196\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 3.2337 - val_accuracy: 0.5196\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.2318 - val_accuracy: 0.5196\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 3.2341 - val_accuracy: 0.5160\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.2324 - val_accuracy: 0.5196\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 3.2342 - val_accuracy: 0.5196\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 3.2309 - val_accuracy: 0.5196\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.2305 - val_accuracy: 0.5196\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.2333 - val_accuracy: 0.5160\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.2345 - val_accuracy: 0.5160\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.2349 - val_accuracy: 0.5160\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.2351 - val_accuracy: 0.5160\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 3.2328 - val_accuracy: 0.5160\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.2341 - val_accuracy: 0.5160\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.2355 - val_accuracy: 0.5231\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 3.2365 - val_accuracy: 0.5196\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.2391 - val_accuracy: 0.5160\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.2404 - val_accuracy: 0.5125\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.2414 - val_accuracy: 0.5160\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.2389 - val_accuracy: 0.5196\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.2388 - val_accuracy: 0.5196\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.2390 - val_accuracy: 0.5231\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.2437 - val_accuracy: 0.5160\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 3.2452 - val_accuracy: 0.5196\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.2412 - val_accuracy: 0.5196\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.2406 - val_accuracy: 0.5267\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.2407 - val_accuracy: 0.5267\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.2403 - val_accuracy: 0.5267\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.2419 - val_accuracy: 0.5302\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.2443 - val_accuracy: 0.5302\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.2443 - val_accuracy: 0.5302\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.2445 - val_accuracy: 0.5267\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.2446 - val_accuracy: 0.5302\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.2456 - val_accuracy: 0.5302\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.2477 - val_accuracy: 0.5302\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.2486 - val_accuracy: 0.5302\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.2508 - val_accuracy: 0.5267\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.2492 - val_accuracy: 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_lstm9/assets\n"
     ]
    }
   ],
   "source": [
    "for s in range(0,10):\n",
    "    adam=tf.keras.optimizers.Adam(learning_rate=0.0001)#learning_rate=0.0005\n",
    "    cnn_lstm=attention_vgg_residual_inception(input_shape, filters, num_classes)\n",
    "    cnn_lstm.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) # optimizer = 'adam'\n",
    "    callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40,restore_best_weights = True)\n",
    "    history=cnn_lstm.fit( X_all_shuffled[s].reshape(1404, 19, 1280,1),# \n",
    "                         to_categorical(Y_all_shuffled[s]),epochs=200, batch_size=200, validation_split=0.2, callbacks=[callback],verbose=1)   \n",
    "    os.chdir('/home/jupy/ICPR_2dCNN_LSTM')\n",
    "    tf.keras.models.save_model(cnn_lstm,filepath=f'cnn_lstm{s}',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f18090de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5512820512820513\n",
      "16.019424 13.2406645\n",
      "16.097164 17.808603\n",
      "15.051967 15.209022\n",
      "17.262754 14.928504\n",
      "15.838656 14.749247\n",
      "9.290916 11.929638\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5705128205128205\n",
      "12.573651 11.40261\n",
      "8.510809 10.131884\n",
      "8.066051 17.321249\n",
      "11.074526 13.668476\n",
      "10.696831 12.148255\n",
      "16.228086 14.322377\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5961538461538461\n",
      "18.727907 21.3219\n",
      "20.65373 21.599699\n",
      "21.845848 23.026905\n",
      "22.871357 19.196354\n",
      "20.198576 18.09425\n",
      "22.50776 24.504707\n",
      "score 0.8333333333333334\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "ep score 0.4935897435897436\n",
      "6.8303785 10.411402\n",
      "3.4609041 4.940084\n",
      "4.387194 4.476521\n",
      "5.650963 6.9139524\n",
      "3.8953614 3.8461335\n",
      "4.982404 9.7973385\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "ep score 0.47435897435897434\n",
      "24.974733 25.30495\n",
      "25.885124 25.99798\n",
      "24.758255 25.96505\n",
      "25.896214 25.70258\n",
      "25.030184 25.967617\n",
      "25.298313 25.53896\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5641025641025641\n",
      "4.4909725 13.184989\n",
      "13.723208 17.108297\n",
      "12.421823 10.908646\n",
      "11.878296 13.147503\n",
      "15.0758095 12.145872\n",
      "16.317537 11.953291\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.46153846153846156\n",
      "0.5039173 0.16430381\n",
      "2.8263931 1.0178796\n",
      "1.0023454 1.0274899\n",
      "1.6073859 1.3006328\n",
      "0.49990523 0.5167992\n",
      "1.69498 1.2487042\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5192307692307693\n",
      "0.12086177 2.2337148\n",
      "0.7309287 3.375405\n",
      "1.2232597 2.6382666\n",
      "1.0891157 0.115610406\n",
      "1.7363539 2.871435\n",
      "1.0026025 1.8573773\n",
      "score 0.6666666666666666\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.46153846153846156\n",
      "17.085333 13.685738\n",
      "20.4588 17.55451\n",
      "16.576103 19.797623\n",
      "15.600104 18.290747\n",
      "18.88633 16.370975\n",
      "13.5165 12.715224\n",
      "score 0.5\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "ep score 0.5769230769230769\n",
      "3.1697936 3.3925984\n",
      "1.3147436 0.044882614\n",
      "0.5562685 1.203053\n",
      "1.5745839 1.2537067\n",
      "3.405322 1.4540502\n",
      "1.6166831 2.8828282\n",
      "score 0.6666666666666666\n",
      "0.6333333333333334\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/jupy/ICPR_2dCNN_LSTM')\n",
    "avg_score=0\n",
    "for s in range(0,10):\n",
    "    classifier=load_model(f'cnn_lstm{s}')\n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "    avg_score=avg_score+accuracy_score(pred,y_test_short)\n",
    "    print('score',accuracy_score(pred,y_test_short))\n",
    "print(avg_score/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f86814",
   "metadata": {},
   "source": [
    "# 1d CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7915bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in range(0,10):\n",
    "    adam=tf.keras.optimizers.Adam(learning_rate=0.001)#learning_rate=0.0005\n",
    "    \n",
    "    def OneD_EEGNet(nb_classes=2, Chans = 19, Samples = number_of_tps, \n",
    "                 dropoutRate = 0.25, kernLength = 64, F1 = 8, \n",
    "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "\n",
    "        input1   = Input(shape = (1217*8, 1))\n",
    "\n",
    "        ##################################################################\n",
    "        block1       = Conv1D(8, (64))(input1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = Activation('relu')(block1)\n",
    "\n",
    "        block1       = Conv1D(16, (32))(block1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = Activation('relu')(block1)\n",
    "\n",
    "        block1       = AveragePooling1D((4))(block1)\n",
    "        block1       = Dropout(dropoutRate)(block1)\n",
    "\n",
    "        block2       = Conv1D(32, (16))(block1)\n",
    "        block2       = BatchNormalization()(block2)\n",
    "        block2       = Activation('relu')(block2)\n",
    "\n",
    "        block2       = AveragePooling1D((8))(block2)\n",
    "        #block2       = tf.keras.layers.GlobalAveragePooling2D()(block2)\n",
    "        block2       = Dropout(dropoutRate)(block2)\n",
    "\n",
    "        flatten      = Flatten(name = 'flatten')(block2)\n",
    "        initializer = initializer = tf.keras.initializers.Identity()\n",
    "        dense        = Dense(nb_classes, name = 'dense', \n",
    "                             kernel_constraint = max_norm(norm_rate),kernel_initializer=initializer)(flatten)\n",
    "        softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "\n",
    "        return Model(inputs=input1, outputs=softmax)    \n",
    "    \n",
    "    model=OneD_EEGNet()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) # optimizer = 'adam'\n",
    "    callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,restore_best_weights = True)\n",
    "    history=model.fit( X_all_shuffled[s].reshape(1404, 9736,1),\n",
    "                          to_categorical(Y_all_shuffled[s]),epochs=200, batch_size=200, validation_split=0.2, callbacks=[callback],verbose=1)   \n",
    "    os.chdir('/home/jupy/ICPR_cnn')\n",
    "    #tf.keras.models.save_model(model,filepath=f'set{s}_btw_encoder_T15_classifier',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n",
    "    tf.keras.models.save_model(model,filepath=f'cnn{s}',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e47508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/ICPR_cnn')\n",
    "avg_score=0\n",
    "for s in range(0,10):\n",
    "    classifier=load_model(f'cnn{s}')\n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "    avg_score=avg_score+accuracy_score(pred,y_test_short)\n",
    "    print('score',accuracy_score(pred,y_test_short))\n",
    "print(avg_score/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8c264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7d85fe",
   "metadata": {},
   "source": [
    "# --------------------- TOPO plot --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#print(mm.get_layer('spatial_conv').get_weights()[0].shape)\n",
    "#mm.get_layer('spatial_conv').get_weights()[0][:,0,1,1]# \n",
    "#height,width, 8 previous outputs,8 feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec85d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "ch_names=['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'Fz', 'Cz', 'Pz', 'A1', 'A2', 'T5', 'T6']\n",
    "info=mne.create_info(ch_names,128,ch_types='eeg')\n",
    "\n",
    "montage=mne.channels.make_standard_montage('standard_1020')\n",
    "info.set_montage(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(mm.get_layer('spatial_conv').get_weights()[0], axis=2)[:,:,0].shape\n",
    "names=ch_names\n",
    "for n in range(0,60):\n",
    "    os.chdir('/home/jupy/SavedModel/autoencoder/T15_to_T15')\n",
    "    mm=load_model(f'T15_to_T15_VAE{n}')\n",
    "    print(n)\n",
    "    mne.viz.plot_topomap(np.sum(mm.get_layer('spatial_conv').get_weights()[0], axis=2)[:,:,7].reshape(19,),info, ch_type='eeg', names=names,size=3, res=128,show=True,\n",
    "                    cmap='viridis',mask_params=dict(marker='o', markerfacecolor='w', markeredgecolor='k',linewidth=0, markersize=4))\n",
    "    print('        -----------------          ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=ch_names\n",
    "for n in range(0,60):\n",
    "    os.chdir('/home/jupy/SavedModel/autoencoder/T15_to_T15')\n",
    "    mm=load_model(f'T15_to_T15_VAE{n}')\n",
    "    print(n)\n",
    "    mne.viz.plot_topomap(np.sum((np.sum(mm.get_layer('spatial_conv').get_weights()[0], axis=2)),axis=2).reshape(19,),info, ch_type='eeg', names=names,size=3, res=128,show=True,\n",
    "                    cmap='viridis',mask_params=dict(marker='o', markerfacecolor='w', markeredgecolor='k',linewidth=0, markersize=4))\n",
    "    #plt.gcf()\n",
    "    #plt.savefig(f'topo{n}', dpi=300)\n",
    "    print('        -----------------          ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ee944",
   "metadata": {},
   "source": [
    "### Average topo for group and color bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b363de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/SavedModel/autoencoder/T15_to_T15')\n",
    "mm_ob=np.zeros(19,)\n",
    "for i in [0,1,2]:\n",
    "    mm=load_model(f'T15_to_T15_VAE{i}') \n",
    "    mm_ob=mm_ob+np.sum((np.sum(mm.get_layer('spatial_conv').get_weights()[0], axis=2)),axis=2).reshape(19,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03589179",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_l=np.zeros(19,)\n",
    "for i in [49,53,57]: # the index of subjects selected for visualise\n",
    "    mm=load_model(f'T15_to_T15_VAE{i}') \n",
    "    mm_l=mm_l+np.sum((np.sum(mm.get_layer('spatial_conv').get_weights()[0], axis=2)),axis=2).reshape(19,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e3b25",
   "metadata": {},
   "source": [
    "#### color bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 35})\n",
    "\n",
    "x3=abs(x1-x2)\n",
    "fig,ax3 = plt.subplots(figsize=(10, 6))\n",
    "#plt.subplots_adjust(wspace=0.6)\n",
    "\n",
    "im,cm   = mne.viz.plot_topomap(x3, info, ch_type='eeg', names=names,axes=ax3,size=16,show=False)  \n",
    "# manually fiddle the position of colorbar\n",
    "ax_x_start = 0.8\n",
    "ax_x_width = 0.02\n",
    "ax_y_start = 0.25\n",
    "ax_y_height = 0.5\n",
    "cbar_ax = fig.add_axes([ax_x_start, ax_y_start, ax_x_width, ax_y_height])\n",
    "clb = fig.colorbar(im, cax=cbar_ax)\n",
    "for t in clb.ax.get_yticklabels():\n",
    "     t.set_fontsize(16)\n",
    "clb.set_ticks([-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8])\n",
    "os.chdir('/home/jupy')\n",
    "plt.savefig(r'topo_color_bar_abs_diff.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 50})\n",
    "x2=mm_ob/3\n",
    "x1=mm_l/3\n",
    "x3=abs(x1-x2)\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(18, 10.8))\n",
    "#plt.subplots_adjust(wspace=0.6)\n",
    "\n",
    "im,cm   = mne.viz.plot_topomap(x2, info, ch_type='eeg', names=names,axes=ax2,size=16,show=False)\n",
    "im,cm   = mne.viz.plot_topomap(x1, info, ch_type='eeg', names=names,axes=ax1,size=16,show=False) \n",
    "#im,cm   = mne.viz.plot_topomap(x3, info, ch_type='eeg', names=names,axes=ax3,size=16,show=False,cmap='viridis')  \n",
    "# manually fiddle the position of colorbar\n",
    "ax_x_start = 0.95\n",
    "ax_x_width = 0.02\n",
    "ax_y_start = 0.25\n",
    "ax_y_height = 0.5\n",
    "cbar_ax = fig.add_axes([ax_x_start, ax_y_start, ax_x_width, ax_y_height])\n",
    "clb = fig.colorbar(im, cax=cbar_ax)\n",
    "for t in clb.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "clb.set_ticks([-8,-7,-6,-5,-4,-3,-2,-1,0,1,2,3,4,5,6,7,8])\n",
    "#clb.ax.set_title(unit_label,fontsize=fontsize)\n",
    "plt.gcf()\n",
    "os.chdir('/home/jupy')\n",
    "plt.savefig(r'topo_color_bar.pdf', dpi=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbba2d",
   "metadata": {},
   "source": [
    "## -----------------Mannwhite significace test on classification score---------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ffd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ttest_ind\n",
    "import statistics\n",
    "\n",
    "#eegnet_mod=[0.677,0.677,0.536,0.677,0.677,0.333,0.677,0.677,0.333,0.833]\n",
    "#vae_mod=[0.677,1,1,1,1,1,1,1,1,0.833]\n",
    "\n",
    "eegnet_mod=[0.622,0.533,0.412,0.718,0.564,0.391,0.654,0.718,0.423,0.737]\n",
    "vae_mod=[0.609,1,1,1,1,1,1,1,1,0.763]\n",
    "\n",
    "#print('std-vae',statistics.stdev(vae_mod),'std-eegnet',statistics.stdev(eegnet_mod))\n",
    "U1, p = mannwhitneyu(eegnet_mod,vae_mod)\n",
    "print(U1,p)\n",
    "#res = ttest_ind(eegnet_mod,vae_mod)\n",
    "#res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c20e9",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c678961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0444644f",
   "metadata": {
    "scrolled": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97154d16",
   "metadata": {},
   "source": [
    "# TSNE visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caa7a0",
   "metadata": {},
   "source": [
    "##### visualised normalised raw_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25f3b89c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0138d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0d1c2fa6",
   "metadata": {},
   "source": [
    "#tsne= TSNE(n_components=2, perplexity=15)\n",
    "#tsne_feat=tsne.fit_transform(Adapt_feat_15_15)\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==0)[0],0],tsne_feat[np.where(np.array(Y2)==0)[0],1],alpha=0.3,label=\"Obese\")\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==1)[0],0],tsne_feat[np.where(np.array(Y2)==1)[0],1],alpha=0.3,label=\"Lean\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.gcf()\n",
    "plt.savefig(r'tsne_post_vae.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e191a",
   "metadata": {},
   "source": [
    "##### visuliased VAE features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c75a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93bdaee",
   "metadata": {},
   "source": [
    "#### colored by subejct, select 6 subject for tsne visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba27462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(27,33):\n",
    "for i in [0,1,2,3,56,57,58,59]:\n",
    "    #tsne_ff=tsne.fit_transform(Adapt_feat_15_15[i*60:(i+1)*60,:])\n",
    "    plt.scatter(tsne_feat[i*26:(i+1)*26,0],tsne_feat[i*26:(i+1)*26,1],alpha=0.3)\n",
    "os.chdir('/home/jupy')\n",
    "plt.savefig(r'vae_tsne_subj_color.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c,lab in zip ( [0,1,2,3,56,57,58,59], ['red','red','red','red','green','green','green','green'],\n",
    "                    [\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"lean\",\"lean\",\"lean\",\"lean\"]):\n",
    "    plt.scatter(tsne_feat[i*26:(i+1)*26,0],tsne_feat[i*26:(i+1)*26,1],alpha=0.3,c=c,label=lab)\n",
    "plt.legend(loc=\"upper left\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58cf2f",
   "metadata": {},
   "source": [
    "# Train and Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82199d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06da573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0713fef0",
   "metadata": {},
   "source": [
    "# EEGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEGNet(nb_classes=2, Chans = 19, Samples = number_of_tps, \n",
    "             dropoutRate = 0.25, kernLength = 64, F1 = 8, \n",
    "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    input1   = Input(shape = (Chans, Samples, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',  #  3 \n",
    "                                   input_shape = (Chans, Samples, 1),\n",
    "                                   use_bias = False)(input1)\n",
    "\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    \n",
    "   \n",
    "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('relu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4))(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16),\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('relu')(block2)\n",
    "    \n",
    "    block2       = AveragePooling2D((1, 8))(block2)\n",
    "    #block2       = tf.keras.layers.GlobalAveragePooling2D()(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    initializer = initializer = tf.keras.initializers.Identity()\n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate),kernel_initializer=initializer)(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4fcbb",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ff929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XX_source, XX_target, yy_source_lab, yy_target_lab = train_test_split( Xtv[s], Ytv[s], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_source.shape\n",
    "yy_source_dom=np.zeros([XX_source.shape[0]])\n",
    "yy_target_dom=np.ones([XX_target.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccdbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdb1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def reverse_gradient(x, hp_lambda):    \n",
    "    # Feed-forward operation:\n",
    "    y = tf.identity(x)\n",
    "    \n",
    "    # Back-propagation/gradient-computing operation:\n",
    "    def _flip_gradient(dy):\n",
    "        return tf.math.negative(dy) * hp_lambda, tf.constant(0.)\n",
    "    \n",
    "    return y, _flip_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(tf.keras.layers.Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return reverse_gradient(inputs, self.hp_lambda)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['hp_lambda'] = self.hp_lambda\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd05542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args): \n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds_head_name = 'class_preds' \n",
    "\n",
    "\n",
    "input_size=(19,1280,1)\n",
    "\n",
    "inputs        = Input(shape=input_size) \n",
    "    \n",
    "# feature extractor\n",
    "block1       = Reshape([19,1280,1])(inputs)\n",
    "block1       = Conv2D(16, (1, 64), padding = 'same')(block1 )\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "block1       = Conv2D(16,(19, 1))(block1)\n",
    "\n",
    "#block1       = DepthwiseConv2D((19, 1), use_bias = False, \n",
    "                                   #depth_multiplier = 2,\n",
    "                                   #depthwise_constraint = max_norm(1.))(block1)\n",
    "\n",
    "#block1       = BatchNormalization()(block1)\n",
    "#block1       = Activation('relu')(block1)\n",
    "#block1       = AveragePooling2D((1, 4))(block1)\n",
    "#block1       = Dropout(0.5)(block1)\n",
    "\n",
    "\n",
    "#block1       = SeparableConv2D(16, (1, 16),use_bias = False)(block1)\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "#block1       = AveragePooling2D((1, 8))(block1)\n",
    "shape = K.int_shape(block1 )  \n",
    "hidden1 = Flatten()(block1)\n",
    "hidden1=Dense(128, activation='relu')(hidden1)\n",
    "hidden1       = Dense(64, activation='relu')(hidden1)\n",
    "\n",
    "\n",
    "latent_dim=10\n",
    "x= Dense(16, activation='relu')(hidden1)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z_log_var = z_log_var + 1e-8 \n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) \n",
    "\n",
    "\n",
    "\n",
    "##################################  Decoder ################################## \n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "x = Conv2DTranspose(filters=8,kernel_size=(19, 1),activation='relu',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2DTranspose(filters=8,kernel_size=(1, 64),activation='relu',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "outputs = Conv2DTranspose(filters=1,kernel_size=5,padding='same',name='decoder_output')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_preds = Dense(2, activation='softmax', name=class_preds_head_name)(hidden1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a992b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_lambda = tf.Variable(1.0)\n",
    "\n",
    "\n",
    "domain_preds_head_name = 'domain_preds'\n",
    "\n",
    "d_input = Input(shape=(latent_dim,))\n",
    "x = Dense(128, activation='linear')(d_input)\n",
    "\n",
    "#x = GradientReversal(hp_lambda)(hidden1)\n",
    "#x = Dense(128, activation='linear')(x)\n",
    "x = Dense(64, activation='linear', name=\"do5\")(x)\n",
    "x = Activation(\"relu\", name=\"do6\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "domain_preds = Dense(2, activation='softmax', name=domain_preds_head_name)(x)\n",
    "\n",
    "\n",
    "d_grl = GradientReversal(hp_lambda)(domain_preds)\n",
    "\n",
    "#domain_classification_model = Model(inputs=inputs, outputs=domain_preds)\n",
    "domain_classification_model = Model(inputs=d_input, outputs=domain_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ef301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import UpSampling2D\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define input shape and other hyperparameters\n",
    "input_shape = (19, 1280, 1)\n",
    "latent_dim = 10\n",
    "lambda_val = 1.0\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Define the encoder network\n",
    "inputs = Input(shape=input_shape)\n",
    "block1       = Reshape([19,1280,1])(inputs)\n",
    "block1       = Conv2D(16, (1, 64), padding = 'same')(block1 )\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "block1       = Conv2D(16,(19, 1), padding = 'same')(block1)\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "\n",
    "shape = K.int_shape(block1 )  \n",
    "block1       = AveragePooling2D((1, 8))(block1)\n",
    "hidden1 = Flatten()(block1)\n",
    "hidden1=Dense(128, activation='relu')(hidden1)\n",
    "hidden1       = Dense(64, activation='relu')(hidden1)\n",
    "x= Dense(16, activation='relu')(hidden1)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z_log_var = z_log_var + 1e-8 \n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) \n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Define the decoder network\n",
    "\n",
    "\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "hidden2 = Dense(64, activation='relu')(latent_inputs)\n",
    "hidden2 = Dense(128, activation='relu')(hidden2)\n",
    "hidden2 = Dense(shape[1] * shape[2] * shape[3], activation='relu')(hidden2)\n",
    "hidden2 = Reshape((shape[1], shape[2], shape[3]))(hidden2)\n",
    "block2 = UpSampling2D((1, 1))(hidden2)\n",
    "block2 = Conv2DTranspose(16,(19, 1), padding='same')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Activation('relu')(block2)\n",
    "block2 = Conv2DTranspose(16, (1, 64), padding='same')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Activation('relu')(block2)\n",
    "x = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(block2)\n",
    "\n",
    "\n",
    "\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43839153",
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_true\n",
    "del domain_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b676ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dense, Flatten, Reshape, BatchNormalization, Lambda, Dropout, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Layer\n",
    "import numpy as np\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    def __init__(self, lambda_val, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.lambda_val = K.constant(lambda_val)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.lambda_val\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'lambda_val': self.lambda_val\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Define sampling function for VAE\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define input shape and other hyperparameters\n",
    "input_shape = (19, 1280, 1)\n",
    "latent_dim = 10\n",
    "lambda_val = 1.0\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "\n",
    "# Define the encoder network\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "block1       = Reshape([19,1280,1])(inputs)\n",
    "block1       = Conv2D(16, (1, 64), padding = 'same')(block1 )\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "block1       = Conv2D(16,(19, 1), padding = 'same')(block1)\n",
    "block1       = BatchNormalization()(block1)\n",
    "block1       = Activation('relu')(block1)\n",
    "\n",
    "shape = K.int_shape(block1 )  \n",
    "block1       = AveragePooling2D((1, 8))(block1)\n",
    "hidden1 = Flatten()(block1)\n",
    "hidden1=Dense(128, activation='relu')(hidden1)\n",
    "hidden1       = Dense(64, activation='relu')(hidden1)\n",
    "x= Dense(16, activation='relu')(hidden1)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z_log_var = z_log_var + 1e-8 \n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var]) \n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "#encoder.summary()\n",
    "\n",
    "# Define the decoder network\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "hidden2 = Dense(64, activation='relu')(latent_inputs)\n",
    "hidden2 = Dense(128, activation='relu')(hidden2)\n",
    "hidden2 = Dense(shape[1] * shape[2] * shape[3], activation='relu')(hidden2)\n",
    "hidden2 = Reshape((shape[1], shape[2], shape[3]))(hidden2)\n",
    "block2 = UpSampling2D((1, 1))(hidden2)\n",
    "block2 = Conv2DTranspose(16,(19, 1), padding='same')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Activation('relu')(block2)\n",
    "block2 = Conv2DTranspose(16, (1, 64), padding='same')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Activation('relu')(block2)\n",
    "x = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(block2)\n",
    "decoder = Model(latent_inputs, x, name='decoder')\n",
    "#decoder.summary()\n",
    "\n",
    "# Define the label classifier network\n",
    "y = Dense(32, activation='relu')(z)\n",
    "y = Dense(2, activation='softmax')(y)#num_classes\n",
    "\n",
    "label_classifier = Model(inputs=z, outputs=y, name='label_classifier')\n",
    "label_classifier.summary()\n",
    "\n",
    "# Define the domain classifier network\n",
    "d = GradientReversal(lambda_val)(z)\n",
    "d = Dense(64, activation='relu')(d)\n",
    "d = Dropout(0.5)(d)\n",
    "d = Dense(2, activation='softmax')(d)\n",
    "\n",
    "domain_classifier = Model(inputs=z, outputs=d, name='domain_classifier')\n",
    "domain_classifier.summary()\n",
    "\n",
    "# Define the VAE-DANN model\n",
    "output = decoder(encoder(inputs)[2])\n",
    "print(output.shape)\n",
    "\n",
    "\n",
    "y_pred = label_classifier(encoder(inputs)[2])\n",
    "\n",
    "domain_pred= domain_classifier(encoder(inputs)[2])\n",
    "#y=label_classifier(encoder(inputs)[2])\n",
    "#vae_dann = Model(inputs, [output, domain_output, y_pred], name='vae_dann')\n",
    "\n",
    "reconstruction_loss = K.sum(K.binary_crossentropy(inputs, output), axis=(1, 2, 3))\n",
    "kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "\n",
    "#label_classification_loss = K.binary_crossentropy(y_pred)\n",
    "\n",
    "#y_true = Input(shape=(2,), name='y_true')\n",
    "def label_classification_loss(y_true, y_pred):\n",
    "    #loss = K.binary_crossentropy(y_true, y_pred)\n",
    "    loss = K.square(y_pred - y_true)\n",
    "    return loss\n",
    "\n",
    "\n",
    "#domain_label = Input(shape=(2,), name='domain_label')\n",
    "#domain_pred = domain_classifier(encoder(inputs)[2])\n",
    "#domain_classification_loss = K.categorical_crossentropy(domain_pred)\n",
    "def domain_classification_loss(domain_label, domain_pred):\n",
    "    #loss = K.binary_crossentropy(domain_label, domain_pred)\n",
    "    loss = K.square(domain_label - domain_pred)\n",
    "    return loss\n",
    "\n",
    "def total_loss(inputs,y_true,domain_label):\n",
    "    loss= reconstruction_loss + kl_loss + domain_classification_loss(domain_label, domain_pred)+ lambda_val * label_classification_loss(y_true, y_pred)\n",
    "    return loss\n",
    "#total_loss = reconstruction_loss + kl_loss + domain_classification_loss(domain_label, domain_pred)+ lambda_val * label_classification_loss(y_true, y_pred)\n",
    "                                                           \n",
    "\n",
    "\n",
    "vae_dann.add_loss(total_loss)\n",
    "vae_dann.compile(optimizer='adam', \n",
    "                 loss={'decoder': 'binary_crossentropy', \n",
    "                     'domain_classifier': 'categorical_crossentropy', \n",
    "                       'label_classifier': 'binary_crossentropy'}, \n",
    "                 loss_weights={'decoder': 1.0, 'domain_classifier': 1.0, 'label_classifier': 1.0})\n",
    "            \n",
    "# Define the loss function\n",
    "#def vae_loss(inputs, output, z_mean, z_log_var, y_true, y_pred, d_label, domain_pred):\n",
    "#def vae_loss(inputs, y_true, domain_label):\n",
    "    #reconstruction_loss = keras.losses.binary_crossentropy(K.flatten(inputs), K.flatten(output))\n",
    "    #reconstruction_loss *= np.prod(input_shape)\n",
    "    #kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    #kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    #kl_loss *= -0.5\n",
    "    #class_loss = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    #domain_loss = keras.losses.categorical_crossentropy(domain_label, domain_pred)\n",
    "    #total_loss = K.mean(reconstruction_loss + kl_loss + class_loss + domain_loss)\n",
    "    #return total_loss\n",
    "\n",
    "# Compile the model\n",
    "#vae.compile(optimizer=Adam(lr=0.0001),\n",
    " #           loss=[vae_loss, 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "  #          loss_weights=[1, 1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53669dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_classifier(encoder(X_batch)[2])[:,0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "source_shuffler=shuffle(np.arange(0,XX_source.shape[0]),random_state=0)\n",
    "target_shuffler=shuffle(np.arange(0,XX_target.shape[0]),random_state=0)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('     ')\n",
    "    print('epoch',epoch)\n",
    "    \n",
    "    \n",
    "    num_batches = Xtv[s].shape[0] // batch_size\n",
    "    \n",
    "\n",
    "    \n",
    "    for batch in range(0,9):\n",
    "        X_batch = (np.vstack([XX_source[source_shuffler,:,:][batch*80:(batch+1)*80,:,:],XX_target[target_shuffler,:,:][batch*20:(batch+1)*20,:,:]]))\n",
    "        y_batch = ((np.hstack([yy_source_lab[source_shuffler,][batch*80:(batch+1)*80,],yy_target_lab[target_shuffler,][batch*20:(batch+1)*20,]]))) #to_categorical\n",
    "        d_batch = ((np.hstack([yy_source_dom[source_shuffler,][batch*80:(batch+1)*80,],yy_target_dom[target_shuffler,][batch*20:(batch+1)*20,]])))\n",
    "        print('y_batch=0',sum(y_batch==0), ' y_batch=1',sum(y_batch==1))\n",
    "        print('d_batch=0',sum(d_batch==0), ' d_batch=1',sum(d_batch==1))\n",
    "        # train on the batch\n",
    "        #loss = vae.train_on_batch(inputs=X_batch, output=decoder(encoder(inputs)[2]),\n",
    "         #                        y_true=y_batch,y_pred=label_classifier(encoder(inputs)[2]),\n",
    "          #                       d_true=d_batch,domain_pred=domain_classifier(encoder(inputs)[2]))\n",
    "        loss = vae_dann.train_on_batch(X_batch, \n",
    "                                 y_batch,\n",
    "                                 d_batch)\n",
    "\n",
    "        \n",
    "        \n",
    "    # print the loss for each epoch\n",
    "#def vae_loss(inputs, output, z_mean, z_log_var, y_true, y_pred, d_true, domain_pred):    \n",
    "    loss_names = vae_dann.metrics_names\n",
    "    for loss_value, loss_name in zip(loss, loss_names):\n",
    "        print('{}: {:.4f}'.format(loss_name, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# Train the VAE-DANN model\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "    for i in range(0, lengh, batch_size):\n",
    "        # Train on batch of source domain data\n",
    "        x_batch = (np.vstack([XX_source[source_shuffler,:,:][batch*80:(batch+1)*80,:,:],XX_target[target_shuffler,:,:][batch*20:(batch+1)*20,:,:]]))\n",
    "        y_batch = ((np.hstack([yy_source_lab[source_shuffler,][batch*80:(batch+1)*80,],yy_target_lab[target_shuffler,][batch*20:(batch+1)*20,]]))) #to_categorical\n",
    "        y_batch=to_categorical(y_batch)\n",
    "        d_batch = ((np.hstack([yy_source_dom[source_shuffler,][batch*80:(batch+1)*80,],yy_target_dom[target_shuffler,][batch*20:(batch+1)*20,]])))\n",
    "        d_batch=to_categorical(d_batch)\n",
    "        loss = vae.train_on_batch(x_batch, [x_batch,y_batch,  d_batch])\n",
    "        \n",
    "        # Train on batch of target domain data\n",
    "        #if i < len(X_target):\n",
    "         #   x_batch_target = X_target[i:i+batch_size]\n",
    "          #  domain_labels[:, 1] = 1  # target domain label\n",
    "           # _, _, _, domain_loss_target = vae_dann.train_on_batch(x_batch_target, [x_batch_target, domain_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=inputs\n",
    "mu=z_mean\n",
    "log_var=z_log_var\n",
    "encoder = Model(input_data, [mu, log_var, z])\n",
    "decoder = Model(latent_inputs, outputs)\n",
    "domain_classifier = Model(d_input, d_grl)\n",
    "\n",
    "vae_output = decoder(encoder(input_data)[2])\n",
    "domain_output = domain_classifier(encoder(input_data)[2])\n",
    "vae_dann = Model(input_data, [vae_output, domain_output])\n",
    "\n",
    "\n",
    "def vae_loss(input_data, output_data, mu, log_var):\n",
    "    reconstruction_loss = K.mean(K.sum(K.binary_crossentropy(input_data, output_data), axis=(1, 2)))\n",
    "    kl_loss = -0.5 * K.mean(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "def domain_loss(d_input, d_output):\n",
    "    return K.mean(K.binary_crossentropy(d_input, d_output))\n",
    "\n",
    "vae_optimizer = Adam(lr=0.0005)\n",
    "domain_optimizer = Adam(lr=0.0005)\n",
    "\n",
    "vae_loss_weight = 1.0\n",
    "domain_loss_weight = 1.0\n",
    "vae_dann.compile(optimizer=vae_optimizer,\n",
    "loss=[vae_loss, domain_loss],\n",
    "loss_weights=[vae_loss_weight, domain_loss_weight])\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "source_shuffler=shuffle(np.arange(0,XX_source.shape[0]),random_state=0)\n",
    "target_shuffler=shuffle(np.arange(0,XX_target.shape[0]),random_state=0)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('     ')\n",
    "    print('epoch',epoch)\n",
    "    \n",
    "    \n",
    "    num_batches = Xtv[s].shape[0] // batch_size\n",
    "\n",
    "    \n",
    "    for batch in range(0,9):\n",
    "        X_batch = (np.vstack([XX_source[source_shuffler,:,:][batch*80:(batch+1)*80,:,:],XX_target[target_shuffler,:,:][batch*20:(batch+1)*20,:,:]]))\n",
    "        y_batch = ((np.hstack([yy_source_lab[source_shuffler,][batch*80:(batch+1)*80,],yy_target_lab[target_shuffler,][batch*20:(batch+1)*20,]]))) #to_categorical\n",
    "        d_batch = ((np.hstack([yy_source_dom[source_shuffler,][batch*80:(batch+1)*80,],yy_target_dom[target_shuffler,][batch*20:(batch+1)*20,]])))\n",
    "        print('y_batch=0',sum(y_batch==0), ' y_batch=1',sum(y_batch==1))\n",
    "        print('d_batch=0',sum(d_batch==0), ' d_batch=1',sum(d_batch==1))\n",
    "        # train on the batch\n",
    "        loss = vae_dann.train_on_batch(X_batch, [y_batch, d_batch])\n",
    "        \n",
    "    # print the loss for each epoch\n",
    "    \n",
    "    loss_names = vae_dann.metrics_names\n",
    "    for loss_value, loss_name in zip(loss, loss_names):\n",
    "        print('{}: {:.4f}'.format(loss_name, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = Model(inputs=inputs, outputs=[class_preds, domain_preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e241c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed782f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6a40a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "source_shuffler=shuffle(np.arange(0,XX_source.shape[0]),random_state=0)\n",
    "target_shuffler=shuffle(np.arange(0,XX_target.shape[0]),random_state=0)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "#X_train=shuffle( np.vstack([XX_source,XX_target]), random_state=0)\n",
    "#y_train=shuffle((to_categorical(np.hstack([yy_source_lab,yy_target_lab]))), random_state=0)\n",
    "#d_train=shuffle( to_categorical(np.hstack([yy_source_dom,yy_target_dom]),2) , random_state=0)\n",
    "\n",
    "dann=combined_model\n",
    "\n",
    "dann.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss={\n",
    "        class_preds_head_name:  'sparse_categorical_crossentropy',\n",
    "        domain_preds_head_name: 'sparse_categorical_crossentropy'},\n",
    "    loss_weights={\n",
    "        class_preds_head_name:  1, \n",
    "        domain_preds_head_name: 1}, \n",
    "    metrics={\n",
    "        class_preds_head_name:  tf.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        domain_preds_head_name: tf.metrics.SparseCategoricalAccuracy(name='acc')},)\n",
    "    #weight_metrics=True  )# not implemented yet. see https://github.com/keras-team/keras/pull/7482 )\n",
    "#dann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('     ')\n",
    "    print('epoch',epoch)\n",
    "    \n",
    "    \n",
    "    num_batches = Xtv[s].shape[0] // batch_size\n",
    "    \n",
    "    \n",
    "    #for i in range(num_batches):\n",
    "\n",
    "        #start_idx = i * batch_size\n",
    "        #end_idx = (i + 1) * batch_size\n",
    "        #X_batch = (np.vstack([XX_source[source_shuffler,:,:],XX_target[target_shuffler,:,:]]))[start_idx:end_idx,]\n",
    "        #y_batch = ((np.hstack([yy_source_lab[source_shuffler,],yy_target_lab[target_shuffler,]])))[start_idx:end_idx,] #to_categorical\n",
    "        #d_batch = ((np.hstack([yy_source_dom[source_shuffler,],yy_target_dom[target_shuffler,]])))[start_idx:end_idx,]\n",
    "    \n",
    "    for batch in range(0,9):\n",
    "        X_batch = (np.vstack([XX_source[source_shuffler,:,:][batch*80:(batch+1)*80,:,:],XX_target[target_shuffler,:,:][batch*20:(batch+1)*20,:,:]]))\n",
    "        y_batch = ((np.hstack([yy_source_lab[source_shuffler,][batch*80:(batch+1)*80,],yy_target_lab[target_shuffler,][batch*20:(batch+1)*20,]]))) #to_categorical\n",
    "        d_batch = ((np.hstack([yy_source_dom[source_shuffler,][batch*80:(batch+1)*80,],yy_target_dom[target_shuffler,][batch*20:(batch+1)*20,]])))\n",
    "        print('y_batch=0',sum(y_batch==0), ' y_batch=1',sum(y_batch==1))\n",
    "        print('d_batch=0',sum(d_batch==0), ' d_batch=1',sum(d_batch==1))\n",
    "        # train on the batch\n",
    "        loss = dann.train_on_batch(X_batch, [y_batch, d_batch])\n",
    "        \n",
    "    # print the loss for each epoch\n",
    "    \n",
    "    loss_names = dann.metrics_names\n",
    "    for loss_value, loss_name in zip(loss, loss_names):\n",
    "        print('{}: {:.4f}'.format(loss_name, loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2b9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1ce60b07",
   "metadata": {},
   "source": [
    "from sklearn.utils import shuffle\n",
    "#dann.metrics_names\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "X_train=shuffle( np.vstack([XX_source,XX_target]), random_state=0)\n",
    "y_train=shuffle((to_categorical(np.hstack([yy_source_lab,yy_target_lab]))), random_state=0)\n",
    "d_train=shuffle( to_categorical(np.hstack([yy_source_dom,yy_target_dom]),2) , random_state=0)\n",
    "\n",
    "dann=combined_model\n",
    "dann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle the training data\n",
    "\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    print('indices',indices) # (1404,)\n",
    "    print('ind_shap',indices.shape)  #(1404, 19, 1280)\n",
    "    X_train = X_train[indices]\n",
    "    print('X_train',X_train.shape)\n",
    "    y_train = y_train[indices]\n",
    "    d_train = d_train[indices]\n",
    "    \n",
    "    \n",
    "    # train on each batch9\n",
    "    num_batches = len(X_train) // batch_size\n",
    "    print('num_batches',num_batches)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # select a batch of data\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        X_batch = X_train[start_idx:end_idx,]\n",
    "        y_batch = y_train[start_idx:end_idx,]\n",
    "        d_batch = d_train[start_idx:end_idx,]\n",
    "\n",
    "        # train on the batch\n",
    "        loss = dann.train_on_batch(X_batch, [y_batch, d_batch])\n",
    "        \n",
    "    # print the loss for each epoch\n",
    "    \n",
    "    loss_names = dann.metrics_names\n",
    "    for loss_value, loss_name in zip(loss, loss_names):\n",
    "        print('{}: {:.4f}'.format(loss_name, loss_value))\n",
    "    \n",
    "    #print(\"Epoch {}: loss = {}\".format(epoch+1, loss))\n",
    "    #loss_names = list(dann.loss.keys())\n",
    "\n",
    "    # Iterate over the loss values and names using zip()\n",
    "    \n",
    "    #for loss_value, loss_name in zip(loss, loss_names):\n",
    "     #   print(loss_name + \": \" + str(loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3e112",
   "metadata": {},
   "source": [
    "#####  change weights of domain classifier and label classifier\n",
    "#####  balance the number source and target domain samples in each mini batch\n",
    "#####  change lambda value of GRL layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a91acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for n in range(0,6):\n",
    "    Result=[]\n",
    "    for i in   dann.predict(Xtest[s][n*26:(n+1)*26,:,:])[0][:,0]:\n",
    "    #for i in  baseline_model.predict(Xtest[0][n*26:(n+1)*26,:,:])[:,0]:\n",
    "        if i>0.5:\n",
    "            Result.append(0)\n",
    "        else:\n",
    "            Result.append(1)\n",
    "\n",
    "    #accuracy_score(Result, target_data_for_class_y)\n",
    "    print('n',n,'score',accuracy_score(Result, (Ytest[s][n*26:(n+1)*26]))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b56b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d1239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dann_out_m = Model(inputs=dann.input,outputs=dann.get_layer('flatten_33').output)\n",
    "dann_out= dann_out_m.predict(XX_0)\n",
    "\n",
    "tsne= TSNE(n_components=2, perplexity=15)\n",
    "tsne_feat_dann=tsne.fit_transform(dann_out)\n",
    "\n",
    "for i in [2,5,18,21,24,31,40,56]:\n",
    "    plt.scatter(tsne_feat_dann[i*26:(i+1)*26,0],\n",
    "                tsne_feat_dann[i*26:(i+1)*26,1],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c,lab in zip ( [2,5,18,21,24,31,40,56], ['red','red','red','red','red','green','green','green'],\n",
    "                    [\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"lean\",\"lean\",\"lean\"]):\n",
    "    plt.scatter(tsne_feat_dann[i*26:(i+1)*26,0],tsne_feat_dann[i*26:(i+1)*26,1],alpha=0.3,c=c,label=lab)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91083b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtv[s].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35950274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_baseline_model(input_shape=(19,1280,1), num_classes=2):\n",
    "    # input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # feature extractor\n",
    "    block1       = Conv2D(16, (1, 64), padding = 'same')(input_layer)\n",
    "\n",
    "    block1       = BatchNormalization()(block1)\n",
    "\n",
    "    block1       = Conv2D(32,(19, 1))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('relu')(block1)\n",
    "    block1       = AveragePooling2D((1, 4))(block1)\n",
    "    block1       = Dropout(0.5)(block1)\n",
    "    \n",
    "    flatten = Flatten()(block1)\n",
    "    print(flatten.shape)\n",
    "         \n",
    "    # label classifier\n",
    "\n",
    "    fc1 = Dense(128, activation='relu')(flatten)\n",
    "    fc2 = Dense(64, activation='relu')(fc1)\n",
    "    label_output = Dense(2, activation='softmax', name='label_output')(fc2)\n",
    "\n",
    "    \n",
    "    # combined model\n",
    "    bl_model = Model(inputs=input_layer, outputs=label_output)\n",
    "    \n",
    "    # compile the model\n",
    "    bl_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #1 is the label classification loss, 2 is the domain loss\n",
    "                       loss={'label_output': 'categorical_crossentropy'},\n",
    "                       metrics={'label_output': 'accuracy'})\n",
    "    \n",
    "    return bl_model\n",
    "\n",
    "#EEGNET=EEGNet()\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "baseline_model=build_baseline_model()\n",
    "baseline_model.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) # optimizer = 'adam'\n",
    "#callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "EEGNET.fit(Xtv[s].reshape(1404,19,1280 ,1),\n",
    "                      to_categorical(np.array(Ytv[s])),epochs=100, batch_size=100, validation_split=0.2, callbacks=[callback],verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res =baseline_model.evaluate(Xtest[0] , to_categorical(Ytest[0]))\n",
    "for n in range(0,6):\n",
    "    Result=[]\n",
    "    #for i in  dann.predict(target_data_for_train)[0][:,0]:\n",
    "    for i in  baseline_model.predict(Xtest[s][n*26:(n+1)*26,:,:])[:,0]:\n",
    "        if i>0.5:\n",
    "            Result.append(0)\n",
    "        else:\n",
    "            Result.append(1)\n",
    "\n",
    "    #accuracy_score(Result, target_data_for_class_y)\n",
    "    print('n',n,'score',accuracy_score(Result, (Ytest[s][n*26:(n+1)*26]))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbc50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_out_m = Model(inputs=baseline_model.input,outputs=baseline_model.get_layer('flatten_31').output)\n",
    "baseline_model_out= bl_out_m.predict(XX_0)\n",
    "\n",
    "tsne_feat_bl=tsne.fit_transform(baseline_model_out)\n",
    "\n",
    "for i in [2,5,18,21,24,31,40,56]:\n",
    "    plt.scatter(tsne_feat_bl[i*26:(i+1)*26,0],\n",
    "                tsne_feat_bl[i*26:(i+1)*26,1],alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c,lab in zip ( [2,5,18,21,24,31,40,56], ['red','red','red','red','red','green','green','green'],\n",
    "                    [\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"Obese\",\"lean\",\"lean\",\"lean\"]):\n",
    "    plt.scatter(tsne_feat_bl[i*26:(i+1)*26,0],tsne_feat_bl[i*26:(i+1)*26,1],alpha=0.3,c=c,label=lab)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a4d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3eeb7f2c",
   "metadata": {},
   "source": [
    "batch_size = 100\n",
    "num_epochs = 25\n",
    "\n",
    "X_train=Xtv[0].reshape(1404,19,1280,1)\n",
    "y_train=to_categorical(Ytv[0])\n",
    "#baseline_model=build_baseline_model()\n",
    "combined_model=combined_model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle the training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "    # train on each batch\n",
    "    num_batches = len(X_train) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        # select a batch of data\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        X_batch = X_train[start_idx:end_idx]\n",
    "        y_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "        \n",
    "        # train on the batch\n",
    "        \n",
    "        loss = combined_model.train_on_batch(X_batch, y_batch)\n",
    "        \n",
    "    # print the loss for each epoch\n",
    "    \n",
    "    print(\"Epoch {}: loss = {}\".format(epoch+1, loss))\n",
    "    loss_names = list(baseline_model.loss.keys())\n",
    "\n",
    "    # Iterate over the loss values and names using zip()\n",
    "    for loss_value, loss_name in zip(loss, loss_names):\n",
    "        print(loss_name + \": \" + str(loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d99026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =baseline_model.evaluate(Xtest[0] , to_categorical(Ytest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a90ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11fc7636",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eegnet_out_m = Model(inputs=EEGNET.input,outputs=EEGNET.get_layer('depthwise_conv2d').output)\n",
    "eegnet_out= eegnet_out_m.predict(XX_15)\n",
    "eegnet_out.shape\n",
    "eegnet_Out=np.zeros([1560,1280*16])\n",
    "for i in range(0,eegnet_out.shape[0]):\n",
    "    eegnet_Out[i,:]=eegnet_out[i,:,:,:].flatten()   \n",
    "    #plt.gcf()\n",
    "os.chdir('/home/jupy')\n",
    "plt.savefig(r'eegnet_tsne_plot.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    " eegnet_Out.shape\n",
    "    \n",
    "tsne= TSNE(n_components=2, perplexity=15)\n",
    "tsne_feat=tsne.fit_transform( eegnet_Out)\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==0)[0],0],tsne_feat[np.where(np.array(Y2)==0)[0],1],alpha=0.3,label=\"Obese\")\n",
    "plt.scatter(tsne_feat[np.where(np.array(Y2)==1)[0],0],tsne_feat[np.where(np.array(Y2)==1)[0],1],alpha=0.3,label=\"Lean\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.gcf()\n",
    "os.chdir('/home/jupy')\n",
    "#plt.savefig(r'tsne_eegnet.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(27,33):\n",
    "for i in [0,1,2,3,56,57,58,59]:\n",
    "    plt.scatter(tsne_feat[i*26:(i+1)*26,0],tsne_feat[i*26:(i+1)*26,1],alpha=0.3)\n",
    "plt.savefig(r'eegNet_tsne_subj_color.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/home/jupy')\n",
    "\n",
    "#np.savetxt('vae_features.txt', (Adapt_feat_15_15))\n",
    "#np.savetxt('eegnet_features.txt', (eegnet_Out))\n",
    "#np.savetxt('labels.txt', (Y2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e85bbf",
   "metadata": {},
   "source": [
    "# ---------------------Train Test split continued-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b00068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_id_long={}\n",
    "X_all_shuffled={}\n",
    "Y_all_shuffled={}\n",
    "Y_all_shuffled_cat={}\n",
    "for s in range(0,10):\n",
    "    np.random.seed(2)\n",
    "    all_id_long[s]=np.arange(0,Xtv[s].shape[0])\n",
    "    np.random.shuffle(all_id_long[s])\n",
    "    \n",
    "    X_all_shuffled[s]=np.zeros([ Xtv[s].shape[0], Xtv[s].shape[1]])\n",
    "    Y_all_shuffled[s]=np.zeros([ Xtv[s].shape[0]])\n",
    "    for n in range(0,all_id_long[s].shape[0]):\n",
    "        X_all_shuffled[s][n,:]=Xtv[s][all_id_long[s][n],:]\n",
    "        #print(n,all_id_long[s][n])\n",
    "        Y_all_shuffled[s][n]=Ytv[s][all_id_long[s][n]]\n",
    "    print(X_all_shuffled[s].shape)\n",
    "    print(Y_all_shuffled[s].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_auto_feat_15_15.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd264a",
   "metadata": {},
   "source": [
    "# 1D CNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneD_EEGNet(nb_classes=2, Chans = 19, Samples = number_of_tps, \n",
    "             dropoutRate = 0.25, kernLength = 64, F1 = 8, \n",
    "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "\n",
    "    input1   = Input(shape = (1217*8, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv1D(8, (64))(input1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('relu')(block1)\n",
    "   \n",
    "    block1       = Conv1D(16, (32))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('relu')(block1)\n",
    "    \n",
    "    block1       = AveragePooling1D((4))(block1)\n",
    "    block1       = Dropout(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = Conv1D(32, (16))(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('relu')(block2)\n",
    "    \n",
    "    block2       = AveragePooling1D((8))(block2)\n",
    "    #block2       = tf.keras.layers.GlobalAveragePooling2D()(block2)\n",
    "    block2       = Dropout(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    initializer = initializer = tf.keras.initializers.Identity()\n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate),kernel_initializer=initializer)(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input1, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac64239",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneD_EEGNet().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5c042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in range(0,10):\n",
    "    lr=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.0005, decay_steps=50, decay_rate=0.9, staircase=False, name=None\n",
    "    )\n",
    "    adam=tf.keras.optimizers.Adam(learning_rate=0.001)#learning_rate=0.0005\n",
    "    \n",
    "    def OneD_EEGNet(nb_classes=2, Chans = 19, Samples = number_of_tps, \n",
    "                 dropoutRate = 0.25, kernLength = 64, F1 = 8, \n",
    "                 D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "\n",
    "        input1   = Input(shape = (1217*8, 1))\n",
    "\n",
    "        ##################################################################\n",
    "        block1       = Conv1D(8, (64))(input1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = Activation('relu')(block1)\n",
    "\n",
    "        block1       = Conv1D(16, (32))(block1)\n",
    "        block1       = BatchNormalization()(block1)\n",
    "        block1       = Activation('relu')(block1)\n",
    "\n",
    "        block1       = AveragePooling1D((4))(block1)\n",
    "        block1       = Dropout(dropoutRate)(block1)\n",
    "\n",
    "        block2       = Conv1D(32, (16))(block1)\n",
    "        block2       = BatchNormalization()(block2)\n",
    "        block2       = Activation('relu')(block2)\n",
    "\n",
    "        block2       = AveragePooling1D((8))(block2)\n",
    "        #block2       = tf.keras.layers.GlobalAveragePooling2D()(block2)\n",
    "        block2       = Dropout(dropoutRate)(block2)\n",
    "\n",
    "        flatten      = Flatten(name = 'flatten')(block2)\n",
    "        initializer = initializer = tf.keras.initializers.Identity()\n",
    "        dense        = Dense(nb_classes, name = 'dense', \n",
    "                             kernel_constraint = max_norm(norm_rate),kernel_initializer=initializer)(flatten)\n",
    "        softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "\n",
    "        return Model(inputs=input1, outputs=softmax)    \n",
    "    \n",
    "    model=OneD_EEGNet()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) # optimizer = 'adam'\n",
    "    callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,restore_best_weights = True)\n",
    "    history=model.fit( X_all_shuffled[s].reshape(1404, 9736,1),\n",
    "                          to_categorical(Y_all_shuffled[s]),epochs=100, batch_size=100, validation_split=0.2, callbacks=[callback],verbose=1)   \n",
    "    os.chdir('/home/jupy/SavedModel/autoencoder/T15_to_T15/classifier')\n",
    "    #tf.keras.models.save_model(model,filepath=f'set{s}_btw_encoder_T15_classifier',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n",
    "    tf.keras.models.save_model(model,filepath=f'n_set{s}_btw_encoder_T15_classifier',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc84240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/SavedModel/autoencoder/T15_to_T15/classifier')\n",
    "for s in range(0,10):\n",
    "    classifier=load_model(f'n_set{s}_btw_encoder_T15_classifier')\n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "\n",
    "    print('score',accuracy_score(pred,y_test_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e6c9c",
   "metadata": {},
   "source": [
    "### MLP and SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471549d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(0,10):\n",
    "    #classifier=load_model(f'n_set{s}_btw_encoder_T15_classifier')\n",
    "    result=[]\n",
    "    clf_svm.fit(X_all_shuffled[s],Y_all_shuffled[s])\n",
    "    predicted=clf_svm.predict(Xtest[s])\n",
    "    result=predicted\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    \n",
    "    subj_result=[]\n",
    "    for i in range(0,6):\n",
    "        print(sum(result[i*26:(i+1)*26]))\n",
    "        if (sum(result[i*26:(i+1)*26]))>13:\n",
    "            subj_result.append(1)\n",
    "        else:\n",
    "            subj_result.append(0)\n",
    "    #pred=subj_class_prob(result)\n",
    "\n",
    "    print('score',accuracy_score(subj_result,y_test_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jupy/SavedModel/autoencoder/T0_to_T0/classifier')\n",
    "for s in range(0,10):\n",
    "    classifier=load_model(f'n_set{s}_btw_encoder_T0_classifier')\n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "\n",
    "    print('score',accuracy_score(pred,y_test_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e9ab1",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------ "
   ]
  },
  {
   "cell_type": "raw",
   "id": "191e7771",
   "metadata": {},
   "source": [
    "def subj_class_prob_svm(result):\n",
    "    pred=[]\n",
    "    for i in range(0,6):\n",
    "        #print(np.where(np.array(result[i*number_of_epochs:(i+1)*number_of_epochs])==1)[0].shape[0]/number_of_epochs)\n",
    "        #print(sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[0], sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])[1])\n",
    "        if(sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])==1)<(sum(predicted[i*number_of_epochs:(i+1)*number_of_epochs])==0):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd479dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,restore_best_weights = True)\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "for s in range(0,10):\n",
    "    one_m=OneD_EEGNet()\n",
    "    one_m.compile(loss = 'categorical_crossentropy', optimizer =adam,metrics=['accuracy']) \n",
    "    one_m.fit( X_all_shuffled[s].reshape(1404, 9736,1),\n",
    "                          to_categorical(Y_all_shuffled[s]),epochs=100, batch_size=100, validation_split=0.2, callbacks=[callback],verbose=1)   \n",
    "    predicted= one_m.predict(Xtest[s])\n",
    "    result=[]\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if sum(predicted==0)>sum(predicted==1):\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "\n",
    "    print('score',accuracy_score(pred,y_test_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda46ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(0,10):\n",
    "    \n",
    "    result=[]\n",
    "    predicted=classifier.predict(Xtest[s])\n",
    "    for i in range (0,np.array(predicted).shape[0]):\n",
    "            if predicted[i][0]>predicted[i][1]:\n",
    "                result.append([0])\n",
    "            else:\n",
    "                result.append([1])\n",
    "    print('ep score',accuracy_score(result,Ytest[s]))\n",
    "    pred=subj_class_prob(result)\n",
    "\n",
    "    print('score',accuracy_score(pred,y_test_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f77460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bdd48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b403e9c",
   "metadata": {},
   "source": [
    "# Interpretable CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableCNN(torch.nn.Module):  \n",
    "    def __init__(self, classes=2, sampleChannel=19, sampleLength=1280 ,N1=16, d=2,kernelLength=64):\n",
    "        super(InterpretableCNN, self).__init__()\n",
    "        self.pointwise = torch.nn.Conv2d(1,N1,(sampleChannel,1))\n",
    "        self.depthwise = torch.nn.Conv2d(N1,d*N1,(1,kernelLength),groups=N1) \n",
    "        self.activ=torch.nn.ReLU()       \n",
    "        self.batchnorm = torch.nn.BatchNorm2d(d*N1,track_running_stats=False)       \n",
    "        self.GAP=torch.nn.AvgPool2d((1, sampleLength-kernelLength+1))         \n",
    "        self.fc = torch.nn.Linear(d*N1, classes)        \n",
    "        self.softmax=torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputdata):\n",
    "        intermediate = self.pointwise(inputdata)        \n",
    "        intermediate = self.depthwise(intermediate) \n",
    "        intermediate = self.activ(intermediate) \n",
    "        intermediate = self.batchnorm(intermediate)          \n",
    "        intermediate = self.GAP(intermediate)     \n",
    "        intermediate = intermediate.view(intermediate.size()[0], -1) \n",
    "        intermediate = self.fc(intermediate)    \n",
    "        output = self.softmax(intermediate)   \n",
    "\n",
    "        return output  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py39",
   "language": "python",
   "name": "venv_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
